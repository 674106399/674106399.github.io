<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>镜子的掌纹</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-06-15T12:03:01.740Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Tau Jiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文阅读：Web-Scale Responsive Visual Search at Bing</title>
    <link href="http://yoursite.com/2020/06/14/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9AWeb-Scale%20Responsive%20Visual%20Search%20at%20Bing/"/>
    <id>http://yoursite.com/2020/06/14/论文阅读笔记：Web-Scale Responsive Visual Search at Bing/</id>
    <published>2020-06-14T10:39:00.000Z</published>
    <updated>2020-06-15T12:03:01.740Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文中我们介绍了微软 bing所使用的全网通用视觉搜索系统。该系统在索引中容纳数百亿张图像，每一张图像有数千个特征，可以在不到200毫秒内做出响应。为了克服在如此大规模的数据中相关性、延迟和可伸缩性方面的挑战，我们采用了基于各种最新深度学习可视化特性的级联learning-to-rank框架，部署在分布式异构计算平台上。定量和定性实验都表明，我们的系统可以支持 bing 网页和应用程序上的各种应用。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>视觉搜索，或者基于内容的图像检索，是一个热门而且长期的研究领域。给定一张图片，视觉搜索系统会返回一个排好序的视觉近似图片列表。通过将查询图片与返回图片列表的所有已知信息进行关联，就能派生出各种各样的应用，比如，定位一张照片是在哪里拍的；从自拍照里识别时尚物品。因此，业内也对此有浓厚兴趣。</p><p>相关性是视觉搜索的主要目标与度量标准。随着最近深度学习的发展，视觉搜索系统在相关性上也有了增进，变得可以更好地被普通消费者使用。已经有业内人士在视觉搜索系统上做出了探索，但他们的工作更多地关注于某个专门的、垂直的系统的可行性，比如，Pinterest 或 eBay 上的图片，并且缺乏对更高级目标的讨论，如相关性、延迟和存储空间占用。本文中，我们将提供微软 Bing 视觉搜索系统的一个综述，希望能为建立一个相关的、反应敏捷的、可扩展的全网视觉搜索引擎提供思路。据作者所知，这是第一个介绍通用全网视觉搜索引擎的工作。</p><p>全网视觉搜索引擎不仅仅是将现有的视觉搜索方法扩展到更大的数据库。这里的『全网规模』意味着数据库并不局限于一个特定的垂直方向或一个特定网站，而是来自于一个通用网络搜索引擎的爬虫。通常这个数据库包含了上百亿张图片，甚至更多。在这种数量级的数据量下，主要挑战来自于三个方面：</p><ol><li><p>很多在小数据集上可行的方法变得不切实际了。比如，Bag of Visual Words 方法通常需要把倒排索引存进内存里以获得高效的检索。假设每张图有差不多100个特征点，这些倒排索引的数据量就接近4TB了，更不用说还有如何高效地进行聚类和量化来生成合理的视觉单词等问题了。</p></li><li><p>即使能正确地分片，存储可扩展性也依然是一个问题。假设我们每张图只使用一条视觉特征，用4096维的 AlexNet 的 fc7层特征，然后把这些特征分片存储到100台机器上，每台机器仍然需要用1.6TB 来存储特征。要知道这些特征不能存储在一般的硬盘上，否则其低随机访问性能会使得搜索延迟变得无法接受。</p></li><li>现代视觉搜索引擎通常使用 learning-to-rank 架构来充分利用多个特征的信息互补，以获得最好的相关性。在一个全网数据库中，这也会造成另一种延迟问题。即使图片的检索可以并行，查询特征提取、分片实例之间的数据流控制，以及最终数据聚合都需要复杂的算法和工程优化。</li></ol><p>除了以上三点以外，通用搜索引擎还有另一个独立的挑战。一个垂直的、专门的搜索引擎通常有一个元数据具有良好组织的受控的图像数据库。然而，通用视觉搜索引擎并不是这样，元数据经常是无组织的，甚至是不可用的。这就更加强调了理解图片内容的能力。</p><p>换言之，一个同时满足高相关性、低延迟、高存储可扩展性的全网视觉搜索系统是很难的。我们提出用巧妙的工程权衡来解决这一困境。具体来说，我们提出用一个级联的 learning-to-rank 框架来权衡相关性和延迟，采用Product Quantization(PQ)方法来权衡相关性与存储空间占用，用配备了SSD的集群来平衡延迟与存储可扩展性。</p><p>在该框架中，分片的倒排索引被用于过滤掉绝无关联的数据库图片，生成一个候选者列表，然后更具描述力也更吃算力的视觉特征用于对候选者进行进一步的重排序，它们中的顶端候选者被传递给最终一级的排序器。在最终级排序器中，最丰富翔实的视觉特征被用于检索，并喂进 LambdaMART 排序模型来取得查询对象与每个候选图片之间的相似性评分，并根据该评分生成排序列表。</p><h2 id="2-系统概述"><a href="#2-系统概述" class="headerlink" title="2 系统概述"></a>2 系统概述</h2><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/bing_img1.png" alt="bing_img1"></p><p>在深入了解系统是如何工作之前，我们先介绍一下工作流。当一个用户上传了一张他在网上找到的或自己拍的查询图片到 bing 中，视觉上相似的图片和商品会被识别出来供用户了解和购买（比如图一所示）。bing 视觉搜索系统包括了下文介绍的三个主要阶段。图二展示了查询图片如何转换为最终结果图像的一般处理工作流。</p><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/bing_img2.png" alt="bing_img2"></p><p><strong>Query understanding 查询理解</strong>：我们从查询图片提取各种各样的特征来描述它的内容，包括DNN编码器，种类识别特征，人脸识别特征，颜色特重和重复检测特征。我们也会生成一个图片标题，用于鉴别查询图片中的关键概念。然后调用场景触发模型来决定在视觉搜索中是否调用不同的场景。比如，当从查询中检测到了购物意图时，将定向搜索一个特定的内容，在特定的购物细分下对内容进行丰富。图一展示了一个相关商品购物场景的用户界面的例子。Bing 视觉搜索系统返回带有购物信息的结果列表，如商店网站链接和价格。用户可以沿着链接去店铺网站上进行购买。目标检测模型也会运行来检测目标，用户可以通过鼠标单击自动标定的对象来查阅相关商品和相关图片。</p><p><strong>Image retrieval 图像检索</strong>：繁重的工作发生在图像检索模块中，该模块根据提取的特征和意图检索视觉上相似的图像。如之前所介绍的，图像检索过程有一个级联式的框架，把整个过程拆分为 Level-0匹配器，Level-1排序器和 Level-2排序器三个阶段。这个模块的细节在后文会有介绍。在排好序的列表生成好以后，我们进行后处理（postprocessing）操作来根据需要去掉重复结果和成人内容。最终的结果集合会被返回给用户。</p><p><strong>Model training 模型训练</strong>：检索过程中使用的多个模型都需要一个训练阶段。第一，我们的系统中利用了多个DNN模型来提高相关性。每个模型由于不同的训练数据、网络结构和损失函数，能独立地提供互补信息。第二，一个联合的 k-means 算法用于在 Level-0匹配器中建立倒排索引。第三，PQ 用于优化深度模型的服务延迟而不损失太多相关性。目标检测模型对于提升用户体验也有帮助。关于这些模型的细节我们会在后文介绍。</p><h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3 方法"></a>3 方法</h2><p>本章将介绍我们如何处理相关性、延迟和存储可扩展性的细节，以及一些其他额外的特征比如目标检测。</p><h3 id="3-1-Relevance-相关性"><a href="#3-1-Relevance-相关性" class="headerlink" title="3.1 Relevance 相关性"></a>3.1 Relevance 相关性</h3><p>为了优化搜索相关性，我们使用了多种特征，其中DNN 特征效果最显著。我们使用了几个 SOAT 模型，包括 AlexNet, ZFSPPNet, GoogleNet和 ResNet。最后一个隐藏层用于提取深度 embedding 特征。我们用多个数据源来训练这些模型，包括人工标注和从网络上爬的，同时也可能使用开源数据集。我们的训练集广泛地覆盖了日常生活中的上千种主要类别，还针对特定领域（比如购物）收集了深入的训练数据集。我们采用多个损失函数来监督深度模型的训练，比如 softmax loss，pairwise loss 和 triplet loss。在表一中完整总结了我们使用的特征。</p><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/WX20200615-194801@2x.png" alt="WX20200615-194801@2x"></p><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/bing_img3.png" alt="bing_img3"></p><p>图三展示了三个我们所使用的深度模型的例子。图三(a)是一个目标检测模型，在查询图片上对目标进行定位和相应的语义类别分类。图三(b)是一个softmax商品分类网络。我们使用特定领域的存储库（包括时尚物品、家用家居、电子产品等）生成产品类别分类和训练数据。种类集合涵盖了上千个细分种类，例如帽子的十个子类如棒球帽、遮阳帽、软呢帽。我们使用 Inception-BN 网络来获得准确率和延迟的折中。网络使用各种组合的超参数从零开始训练。在视觉搜索排序器中，使用了pool5层的DNN 特征和 softmax 层的种类特征。图三(c)是一个 triplet 网络，直接学习从图片到欧式空间的 embedding，特征点之间的距离对应了图像的相似程度。该网络在一个很大的人工标注的图片三元组$$(Q,I_1,I_2)$$集合上训练，每一个标注者回答I1和 I2哪个跟 Q 更相似。我们用 ImageNet 数据在一个预训练的神经网络上微调这个 triplet 网络，要降低的损失函数为：</p><p>$$L=\sum_{i=1}^{N}(|f(x_i^{query})-f(x_i^{pos})|^2 - |f(x_i^{query})-f(x_i^{neg})|^2 + margin)_+ ….(1)$$</p><p>这里 f(x)表示原本神经网络的最后一个全连接层后面新加的稠密 embedding 层，并且这个 embedding 层被作为视觉搜索中另一个神经网络特征来使用。</p><p>各种各样的神经网络特征、非神经网络特征、目标检测和文本匹配特征被聚合与利用在视觉搜索排序器中。更多的细节在表一中列出。我们也使用局部特征来消除重复图片。我们使用一个 基于ranking loss 的LambdaMART 排序模型来读入特征向量，输出最终排序评分。最终结果图片按评分进行排序。排序器的训练数据收集方式类似于 triplet 模型的训练数据收集。我们还用了一个额外的回归模型用于将一行一行形式的 pair-wise判断转化为 list-wise 判断，作为 LambdaMART 排序模型训练的 ground truth。</p><h3 id="3-2-Latency-延迟"><a href="#3-2-Latency-延迟" class="headerlink" title="3.2 Latency 延迟"></a>3.2 Latency 延迟</h3><p>图片索引包含了上百亿的图片，因此要提供敏捷的用户反馈体验是很具有挑战性的。我们设计了一个级联式的排序框架来达到相关性与延迟的良好平衡。直觉上，过滤掉『无望』的候选者比检索到视觉相关的图片要简单得多。因此，简单特征可以用于显著缩小搜索空间，之后再用复杂而富含信息的特征来进行高质量的重排序。倒排索引可以用于生成初始的候选者集合。</p><p>如图四所示，级联式排序框架包括了 Level-0匹配器，Level-1排序器和 Level-2排序器。Level-0匹配器阶段基于倒排索引生成初始候选者集合。按照Arandjelovic et al.的提出方法，我们使用联合 k-means 训练得到的视觉单词作为图片的简单特征表达，细节如图五所示。量化之后，每张图用 N 个视觉单词来表示。只保留与查询图像的视觉单词相匹配的索引图像，并将其发送给 Level-1排序器。</p><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/bing_img4.png" alt="bing_img4"></p><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/bing_img5.png" alt="bing_img5"></p><p>视觉单词可以在几毫秒内把候选者集合从十亿级缩减到几百万量级，然而这还是太多了。因此我们使用轻量级的 Level-1排序器来进一步减少候选者数量，这一步使用单一的神经网络特征。考虑到特征的存储和计算欧式距离的成本，我们采用了 PQ 方法来节省开支，其中的直觉与视觉词袋方法类似。我们没有对高维向量做处理，而是使用聚类中心来作为近似的特征向量。由于聚类中心之间的距离可以预先计算好，距离计算的时间可以大大缩短。又因为只用存图片ID和一个字典，空间消耗也极大地节省了。图六显示了一个高维向量分解成许多低维子向量，从而形成一个 PQ 向量。首先我们把DNN 特征通过 PCA降维到4n 维。然后每个新的4n 维的向量被分成 n 个4维向量，PQ 模型的最近的聚类中心由每个4维向量决定。最终每个神经网络编码器可以由只占 n 个字节的中心点 ID 表示，从而形成一个 PQ 向量，然后特征距离可以近似地由计算查询图片的 PQ 向量$${PQ_i^q}_{i=1}^n$$和候选图片的 PQ 向量$${PQ_i^c}_{i=1}^n$$的距离得到。要注意每个$$PQ_i^q$$和$$PQ_i^c$$ 的欧式距离可以预先计算并缓存起来。</p><p>作为 Level-1排序器的结果，候选集合可以从百万级减小到数千张图片。在此刻，我们就可以用更加吃计算量的 Level-2排序器来进行更精准的图片排序了。所有 Level-2排序器中用到的DNN 特征也都用了 PQ 加速。</p><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/bing_img6.png" alt="bing_img6"></p><h3 id="3-3-Storage-存储"><a href="#3-3-Storage-存储" class="headerlink" title="3.3 Storage 存储"></a>3.3 Storage 存储</h3><p>所有我们级联式框架中需要的特征都被存入内存以获得高效检索。我们用 PQ IDs 而不是真实数值来表示 DNN 特征，这能极大减小存储需求。例如，一张图片的2048维的 ResNet DNN 编码器的空间需求，通过PCA 和 PQ可以从8KB 降低到25bytes（假如每次4维向量聚类成256个类，PQ 中设置 n=25）。视觉词汇用于减少匹配阶段的存储需求。具体来说，每张图的视觉词汇集合只占用64bytes。最后，我们把所有特征（包括视觉词汇倒排索引和 PQ IDs）分片存储到几百台机器上，来进一步缓解计算和存储压力。并且用上一节中介绍的三层式结构来高效管理分布式索引。</p><h3 id="3-4-目标检测"><a href="#3-4-目标检测" class="headerlink" title="3.4 目标检测"></a>3.4 目标检测</h3><p>我们使用了 SOAT的目标检测技术来为用户提供简单而精确的视觉搜索输入指导。举例来说，如果用户在搜索服装灵感，我们可以预测用户的搜索/购物意图，并自动检测用户感兴趣的多个对象并进行标记，因此用户不再需要摆弄边框（手动裁剪图片）。我们从多种深度学习目标检测框架中选择了两种方案以取得速度和准确性的平衡：Faster R-CNN 和 Single Shot Multi-box Detection(SSD)。在训练中我们实验了各种各样的骨干网络结构和调参来得到准确率和召回率最高的模型。时尚物品和家具是目标检测模型最开始的目标。为节省计算开销，目标检测模型只会当从查询图片中检测到特定的视觉意图时才会运行。目标检测模块除了提供准确的定位 bbox，还提供了目标的类别预测。我们用预测的类别与存在的图片类别进行匹配以进一步提高相关性。</p><h3 id="3-5-工程"><a href="#3-5-工程" class="headerlink" title="3.5 工程"></a>3.5 工程</h3><p>我们绘制了整个视觉搜索处理的全貌，然后发现最耗时的阶段是特征提取和相似图片检索（Level-0, Level-1, Level-2）。因此，我们在这两个模块中进行了工程优化来减少系统延迟和提高开发敏捷度。</p><p><strong>CPU 上特征提取</strong>：由于这个项目在 GPU 加速还没有普及的时候就开始了，我们开发了一个被称为神经网络工具集（NNTK）的库用来优化 CPU 上的DNN 模型部署。所有的核心操作（如矩阵乘法和卷积运算）都用最新版本的汇编语言指令编写。我们还小心优化了卷积层前向传播中不同通道的尺寸和特征图尺寸来使得它们可以被友好地缓存。我们还用了线程级、机器级的并行策略以及负载均衡来进一步提高吞吐量。</p><p>考虑到不同应用场景可能需要不同的特征，为了能灵活地重新组织特征提取 pipeline而不需要费时的重新编译，我们开发了一种Domain Specific Language(DSL)。（PS：后面关于DSL 的介绍不太感兴趣，我就不翻译了）</p><p><strong>GPU 上特征提取</strong>：随着工业级 GPU 加速的 DNN 框架（如 CNTK和 Caffe）出现，我们是最早把图像搜索引擎服务栈部署到GPU 上的团队之一。基于 GPU 的 DNN 推理可以加速特征提取10-20倍（识模型而定）。我们把延迟敏感的模型比如目标检测模型，部署到Azure 的弹性 GPU 集群上，取得了40ms 的延迟。正确地对系统进行缓存还可以进一步降低延迟和开销。</p><p><strong>高效分布式检索</strong>：除了级联式框架外，我们还使用了分布式框架来加速图像检索和重排序系统，其瓶颈主要在于 Level-0匹配器和特征检索。Level-0匹配器的倒排索引检索阶段在上百台机器上并行完成。特征被存储在 SSD 上，性能比存放在普通机械硬盘上好得多。</p><h2 id="4-应用"><a href="#4-应用" class="headerlink" title="4 应用"></a>4 应用</h2><p>（描述了一些应用场景，跟操作指南差不多，没什么重点也不翻译了 hhh）</p><h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h2><p>本章我们提供了本系统的评价指标，按照主要动机，我们使用相关性、延迟和存储消耗来评估一个图像搜索引擎。我们也提供了一些定性结果来显示一般结果质量和系统使用体验。</p><p><strong>相关性</strong>：NDCG 用于评估相关性。从 bing 的搜索历史记录中我们准备了一个几千张查询图片的测试集。用这些查询图片，我们从 Bing和其他搜索引擎拿到了最相关的候选图片。然后我们询问人工标注员来进行候选图片相关性的 pair-wise 判断，基于此来作为排序模型的 ground truth。基于这个测试集，我们用搜索引擎给候选图片排序，并计算 NDCG@5作为最终度量指标。</p><p>这个度量指标有利有弊。一个理想的评估搜索引擎相关性的方式是得到每个查询图片与所有图片的相关性排序，但在百亿级的图片上，这是不可行的。这是在受约束的测试集上计算 NDCG 的优点：这个方案的标注工作量是可接受的。然而当分析评分数值时，我们也应当知道，只有 Level-2排序器模块是用这个方法评估的，前面的模块不影响 NDCG 数值。</p><p>在表二中，我们记录了本系统的 NDCG@5和不同的单一特征 baseline。所有表中列出的 DNN 模型都是在几千个分类的 Bing 数据集上训练得到的。从表中数据我们可以看出，选用的 DNN 特征确实提供了互补信息，提高了相关性。我们也分析了如何取舍存储和相关性对相关性的影响：表三比较了使用行 DNN 特征的系统的 NDCG@5，使用 PCA 降维的 DNN 特征，PCA 顶部用 PQ 量化的 DNN 特征。</p><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/WX20200615-195404@2x.png" alt=""></p><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/bing3.png" alt="bing3"></p><p><strong>延迟</strong>：我们在表四中测量并报告了端到端和组件方式的系统延迟。如第4节所述，从技术角度来看，存在两种不同的方案。 一种是查询图像在我们的索引中，其中视觉发现是预期的用法。 另外一个是用户上传查询，或者当用户指定裁剪框对感兴趣区域进行可视搜索时上传的查询。 在前一种情况下，因为我们已经具有视觉特征，所以可以通过节省网络传输和特征提取时间来减少系统延迟。 后一种情况将显示更长的延迟，因为我们必须进行成熟的特征提取。 从表中可以看出，由于级联重排模块的先进优化，延迟瓶颈实际上是视情况而定的视觉特征的检索或计算。 请注意，当前特征提取仍在CPU上执行，如果使用GPU，则将大大加速。</p><p><img src="/Users/billow/Desktop/674106399.github.io/source/images/bing4.png" alt="bing4"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;本文中我们介绍了微软 bing所使用的全网通用视觉搜索系统。该系统在索引中容纳数百亿张图像，每一张图像有数千个特征，可以在不到200毫秒内做
      
    
    </summary>
    
      <category term="Technology" scheme="http://yoursite.com/categories/Technology/"/>
    
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
      <category term="Image Retrieval" scheme="http://yoursite.com/tags/Image-Retrieval/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：SubSpace Capsule Network</title>
    <link href="http://yoursite.com/2020/04/19/%E8%AE%BA%E6%96%87SubSpace%20Capsule%20Network/"/>
    <id>http://yoursite.com/2020/04/19/论文SubSpace Capsule Network/</id>
    <published>2020-04-19T14:14:53.000Z</published>
    <updated>2020-05-07T07:06:18.266Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文阅读：SubSpace-Capsule-Network"><a href="#论文阅读：SubSpace-Capsule-Network" class="headerlink" title="论文阅读：SubSpace Capsule Network"></a>论文阅读：SubSpace Capsule Network</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文提出了子空间胶囊网络（SCN），该网络应用胶囊网络的思想，通过一组胶囊子空间对实体的外观或隐式定义的属性的可能变化进行建模，而不是简单地将神经元分组来创建胶囊。使用一个可学习的变换，通过将输入特征向量从较低层投影到胶囊子空间上来创建胶囊。 此变换找到输入与胶囊子空间建模的属性的相关程度。</p><p>本文展示的 SCN 是一个通用的胶囊网络，可以被应用于 GAN，在test 阶段不会产生多余的计算开销。通过使用生成对抗网络（GAN）框架对监督图像分类，半监督图像分类和高分辨率图像生成任务进行全面的实验，评估了SCN的有效性。 SCN显着提高了所有3个任务中基准模型的性能。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>一个胶囊是由一组神经元定义的，它们可以完备地对不同属性进行建模，例如姿态、纹理、一个实体的整体或局部形变等。胶囊网络的每一层由很多个胶囊组成。一个训练好的胶囊网络，每个胶囊的激活向量表示实体的实例化参数，且向量的长度表示那个特征或特征组件的存在可能性分数。本文尽管依旧遵循胶囊的主要定义，但提出的子空间胶囊网络是基于一个输入特征向量和一组学好的子空间之间的相关程度。在 SCN 中，每个专属的胶囊子空间是通过相关的实体或实体组件来学习的。然后通过使用基于对应胶囊子空间定义的学习变换，把输入向量投影到胶囊子空间中来创建胶囊。从直觉上讲，一个胶囊子空间捕捉了一个实体或实体组件的视觉属性的变换（如外观，姿态，纹理和形变等），一个子空间的输出特征的长度表示了该输入特征与该子空间所对应属性之间的相关程度。因此，如果一个子空间有一个很大的激活向量，这意味着该输入特征跟该子空间所对应的属性高度相关，反之亦然。这种创建子空间胶囊的形式与基于路由机制的形式是互相独立的，因此可以很容易地应用到大型网络中。</p><p>与本工作最相关的工作是胶囊投影网络（CapProNet），它极少量地应用了基于子空间的胶囊到图像分类网络的最后一层，且只需要胶囊的长度来进行预测。在一个 N 分类任务中，会学习一组胶囊子空间$${S_1,…,S_N}$$ ，每个类的胶囊通过输入骨干网络特征向量到每个子空间的正交投影来创建。输入图片归类到拥有最大胶囊长度的类别上。与 CapProNet 不同的是，SCN 同时对子空间胶囊和胶囊长度进行了研究。以下是本文的贡献：</p><ul><li>SCN 是一种通用的胶囊模型，可以直接应用到生成模型和判别模型中</li><li>SCN 计算量很小，在test 阶段不会增加额外的计算开销；在 train 阶段使用第五节的方法，增加的计算量也微乎其微</li><li>使用了 SCN 的 GAN ，生成的样本有至少20%的 FID score 提升</li><li>在 CIFAR10和 SVH 数据集的半监督分类任务中，SCN 达到了 SOAT，且两个数据集的提升均23%以上</li><li>SCN 很容易迁移到大型网络结构上。当应用到 ResNet 的最后一个 Block 上，Top1错误率有相对于之前5%的提升（PS：这里说法很 tricky，并不是准确率直接提升了5%，而是提升的比例相对于之前是5%）</li></ul><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src="../images/WX20200424-181509@2x.png" alt="WX20200424-181509@2x"></p><p>a) 生成器中，隐式表示 z 在第一层以 c=16维被投影到10个胶囊子空间中，具有最大的向量的胶囊被选中，然后变形为一个25x2x2的立方体，然后上采样来翻倍空间分辨率为4x4.该立方通过2层 sc-conv，每层都进行上采样操作来得到16x16的分辨率。最终的 sc-conv 层具有8个子空间胶囊类型，每个8维，这一层的输出会被变形来生成图片。</p><p>b)判别器中，使用6个卷积层提取特征，后面接3个 sc-conv 层，每层都含有64个子空间胶囊类型，1个子空间胶囊平均池化层和最终一个具有10个胶囊类型的子空间胶囊全连接层(sc-fc)。</p><p>SCN 的关键在于找到输入特征与胶囊子空间之间的相关程度，因此本文设计了投影矩阵和子空间，以及胶囊激活函数和子空间胶囊平均池化。</p><h3 id="胶囊子空间投影"><a href="#胶囊子空间投影" class="headerlink" title="胶囊子空间投影"></a>胶囊子空间投影</h3><p>对于layer k， 假如 x 是一个来自于 layer k-1的 d 维向量。假设一个 c 维的胶囊子空间 S 是由权重矩阵 W (d x c)的列的跨度构成的，c 远小于 d</p><p>最直观的方式来寻找特征向量 x 于胶囊子空间 S 的相关程度的方法，是把 x 正交投影到 S，该问题的一个近似解决方案是：$$y = W(W^TW)^{-1}W^Tx, P=W(W^TW)^{-1}W^T$$ ，P 是到 S 的正交投影矩阵，y 是 x 在 S 的投影。y 的长度越大，x 与 S 越相关，换言之，x 有越多的属性由 S 建模。</p><p>然而，投影矩阵 P 主要的缺点在于是一个正方形矩阵，这意味着如果我们创建一个 d 维特征 x 使用 P投影到 S得到的胶囊，那么这个胶囊也是在 d 维空间里的。实际上，在深度模型中 d 通常是很大的，使用正交投影矩阵 P 得到不同的胶囊类型会需要大量的内存。为了能够通过子空间胶囊层序列从各种大小的胶囊中受益，需要进行一种变换，来允许将输入特征向量 x 映射到胶囊子空间的 c 维空间，同时仍保留网络中连续的层的胶囊之间的关系。我们提出使用变换矩阵$$P_c$$ 代表的中间域来表示胶囊子空间。该矩由分解正交投影矩阵 P 得到：</p><p>$P=P_dP_c….(2)$  </p><p>$P_d=W(W^TW)^{-1/2}, P_c=(W^TW)^{-1/2}W^T$</p><p>这里 $$P_c$$ 是把输入特征 x 映射到 c 维胶囊空间的变换，$$P_d$$ 是让胶囊空间中的投影向量回到原始的 d 维输入向量空间的变换。现在，与胶囊子空间 S 相关的胶囊可以通过将特征向量 x 投影到 c 维胶囊子空间得到：</p><p>$u=P_cx….(4)$</p><p>这里 u 代表 x 在胶囊空间中的低维表示。矩阵 P是一个半定对称矩阵。因此它的分解如式子(2)所示具有特殊属性。本文证明了使用$$P_c$$ 创建的胶囊具有相同的实例化参数信息和特征存在性分数，因为它是由变换 P 得到的，具体证明过程见原文。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>本文基于对胶囊输出向量长度的解释，应用了两种激活函数在子空间胶囊上。胶囊输出向量的长度可以从置信度的角度解释。一个高置信度的胶囊说明输入向量跟该胶囊子空间是高度相关的。换言之，输入向量包含了该胶囊子空间建模的实体。本文也希望通过激活函数来抑制噪声胶囊的效果，从这个角度我们提出了”sparking”函数：</p><p>$v = max(||u||-b^2, 0)\frac{u}{||u||}….(5)$</p><p>b 是一个可学习的参数。</p><p>直觉上，如果 x 跟子空间S 建模的实体相关，sparking 函数会提升胶囊确定性；如果它的长度小于阈值$$b^2$$，则完全关闭胶囊。初始时$$b^2=0.25$$ ，在训练时更新它。</p><p>另一种可能性是将由胶囊子空间建模的实体的存在概率与输出胶囊的长度相关联。对于这种情况，本文遵循 Hinton 提出的”squashing”函数：</p><p>$v = \frac{||u||^2}{1+||u||^2}\frac{u}{||u||}….(6)$</p><p>本文发现 sparking 函数在判别任务中更有效，比如半监督图像分类，因为它通过关闭噪声胶囊输出稀疏特征图加速了收敛。噪声胶囊是指每层中表示的属性与输入图片无关的胶囊，它们仍然会产生一个小的激活向量。然而在生成模型中，通过使用 squashing 激活函数来得到小而非零的值能提高生成样本的质量。</p><h3 id="子空间胶囊卷积"><a href="#子空间胶囊卷积" class="headerlink" title="子空间胶囊卷积"></a>子空间胶囊卷积</h3><p>SCN 也可以从CNN权重共享的思维中受益，通过在图像的所有空间位置上使用相同的子空间胶囊类型。在子空间胶囊卷积中，如果输入 x 有 i 张特征图，而我们希望创建一个 感受野维 k 的c 维的子空间胶囊卷积核，我们需要基于权重矩阵 W（$$(i\times k \times k)\times c$$）构建变换矩阵$$P_c$$ 。我们可以把投影矩阵$$P_c$$的每一行看成一个 $$i \times k\times k$$ 的卷积核，对输入特征图进行卷积并生成输出胶囊的单个元素。所以如果$$P_c$$ 被重组到一个形如$$c \times i\times k \times k$$ 的4维张量，那么它可以被作为标准的卷积操作使用，且对应于每个空间位置的胶囊会被摆放在输出特征图的相应位置。现在，如果我们想有 n 个子空间胶囊类型，我们可以创建一组投影矩阵{$$P_{c1},…,P_{cn}$$} ，把它们分别重组成4维张量后拼接到一起来创建一个形如$$nc \times i \times k \times k$$ 的核，至此，我们使用一个四元组(n,c,k,k)来表示子空间胶囊卷积层</p><h3 id="子空间胶囊平均池化"><a href="#子空间胶囊平均池化" class="headerlink" title="子空间胶囊平均池化"></a>子空间胶囊平均池化</h3><p>平均池化的思想很自然地来自于子空间胶囊卷积。在子空间胶囊卷积中，相同类型的胶囊表示相同的视觉属性不论空间位置。因此可以安全地假设$$k \times k$$ 的小感受野中相同类型胶囊具有相似的方向，并且可以用一个它们的均值计算得到的胶囊来代表它们全部。</p><h2 id="投影矩阵实现"><a href="#投影矩阵实现" class="headerlink" title="投影矩阵实现"></a>投影矩阵实现</h2><p>投影矩阵$$P_c$$包含计算量非常大的操作$$W^TW$$，如果不能正确实现，会极大拖慢训练过程。本文中，使用了Denman-Beavers 迭代方法的稳定扩展。对于任意的正定（半定）矩阵 A，存在一个唯一对称正定（半定）平方根矩阵。在所有训练过程中，我们设置迭代次数 k=20.这只极小地增加训练时间。 而在训练结束后，胶囊投影矩阵$$P_c$$就固定了，不再需要额外的时间。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;论文阅读：SubSpace-Capsule-Network&quot;&gt;&lt;a href=&quot;#论文阅读：SubSpace-Capsule-Network&quot; class=&quot;headerlink&quot; title=&quot;论文阅读：SubSpace Capsule Network&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="Technology" scheme="http://yoursite.com/categories/Technology/"/>
    
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
      <category term="Image Retrieval" scheme="http://yoursite.com/tags/Image-Retrieval/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>线段树Python</title>
    <link href="http://yoursite.com/2019/07/16/%E7%BA%BF%E6%AE%B5%E6%A0%91Python/"/>
    <id>http://yoursite.com/2019/07/16/线段树Python/</id>
    <published>2019-07-16T14:14:53.000Z</published>
    <updated>2019-07-17T04:19:10.371Z</updated>
    
    <content type="html"><![CDATA[<p>最近在参加秋招，头条面试的时候遇到一道题感觉很契合实际场景，唤起了我对线段树的记忆题目是这样的：</p><blockquote><p>给出一份服务器一天的log文件，包含用户id，登录时间，登出时间，求当天服务器最大在线人数是多少</p></blockquote><p>当时看完这个题第一反应就是可以用线段树，但是线段树我以前只是了解了一下概念和适用场景，没有真正仔细研读，趁此机会算是好好学习了一下</p><p>线段树网上的教程一搜一大把，不过很少有Python版本的，能搜到的也都没有区间更新或者是维护节点和，而不是求最大值，所以把我的版本贴出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义树节点，l,r, val表示该节点记录的是区间[l, r]的最大值是val</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tree</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.l = <span class="number">0</span></span><br><span class="line">        self.r = <span class="number">0</span></span><br><span class="line">        self.lazy = <span class="number">0</span></span><br><span class="line">        self.val = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># 二叉树是堆形式，可以用一维数组存储，注意数组长度要开4倍空间</span></span><br><span class="line">tree = [Tree() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>*<span class="number">4</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建树，用cur&lt;&lt;1访问左子树，cur&lt;&lt;1|1访问右子树，位运算操作很方便</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(cur, l, r)</span>:</span></span><br><span class="line">    tree[cur].l, tree[cur].r, tree[cur].lazy, tree[cur].val = l, r, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="comment"># 当l==r的时候结束递归</span></span><br><span class="line">    <span class="keyword">if</span> l &lt; r:</span><br><span class="line">        mid = l + r &gt;&gt; <span class="number">1</span></span><br><span class="line">        build(cur&lt;&lt;<span class="number">1</span>, l, mid)</span><br><span class="line">        build(cur&lt;&lt;<span class="number">1</span>|<span class="number">1</span>, mid+<span class="number">1</span>, r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当子节点计算完成后，用子节点的值来更新自己的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pushup</span><span class="params">(cur)</span>:</span></span><br><span class="line">    tree[cur].val = max(tree[cur&lt;&lt;<span class="number">1</span>].val, tree[cur&lt;&lt;<span class="number">1</span>|<span class="number">1</span>].val)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 单点更新</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(cur, x, v)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> tree[cur].l == tree[cur].r:</span><br><span class="line">        tree[cur].val += v</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        mid = tree[cur].r + tree[cur].l &gt;&gt; <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> x &gt; mid:</span><br><span class="line">            add(cur&gt;&gt;<span class="number">1</span>|<span class="number">1</span>, x, v)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            add(cur&lt;&lt;<span class="number">1</span>, x, v)</span><br><span class="line">        pushup(cur)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将lazy标记向下传递一层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pushdown</span><span class="params">(cur)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> tree[cur].lazy:</span><br><span class="line">        lazy = tree[cur].lazy</span><br><span class="line">        tree[cur&lt;&lt;<span class="number">1</span>].lazy += lazy</span><br><span class="line">        tree[cur&lt;&lt;<span class="number">1</span>|<span class="number">1</span>].lazy += lazy</span><br><span class="line">        tree[cur&lt;&lt;<span class="number">1</span>].val += lazy</span><br><span class="line">        tree[cur&lt;&lt;<span class="number">1</span>|<span class="number">1</span>].val += lazy</span><br><span class="line">        tree[cur].lazy = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 区间更新</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(cur, l, r, v)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> l &lt;= tree[cur].l <span class="keyword">and</span> tree[cur].r &lt;= r:</span><br><span class="line">        tree[cur].lazy += v</span><br><span class="line">        tree[cur].val += v</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> r &lt; tree[cur].l <span class="keyword">or</span> l &gt; tree[cur].r:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> tree[cur].lazy:</span><br><span class="line">        pushdown(cur)</span><br><span class="line">    update(cur&lt;&lt;<span class="number">1</span>, l, r, v)</span><br><span class="line">    update(cur&lt;&lt;<span class="number">1</span>|<span class="number">1</span>, l, r, v)</span><br><span class="line">    pushup(cur)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 区间查询</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query</span><span class="params">(cur, l, r)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> l &lt;= tree[cur].l <span class="keyword">and</span> tree[cur].r &lt;= r:</span><br><span class="line">        <span class="keyword">return</span> tree[cur].val</span><br><span class="line">    <span class="keyword">if</span> tree[cur].l &gt; r <span class="keyword">or</span> tree[cur].r &lt; l:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> tree[cur].lazy:</span><br><span class="line">        pushdown(cur)</span><br><span class="line">    <span class="keyword">return</span> max(query(cur&lt;&lt;<span class="number">1</span>, l, r), query(cur&lt;&lt;<span class="number">1</span>|<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="comment"># -----</span></span><br><span class="line"><span class="comment">#        ---</span></span><br><span class="line"><span class="comment">#  -------</span></span><br><span class="line"><span class="comment">#   --</span></span><br><span class="line"><span class="comment">#         --</span></span><br><span class="line"></span><br><span class="line">build(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">update(<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">update(<span class="number">1</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">update(<span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">update(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">update(<span class="number">1</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">print(query(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回为3</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在参加秋招，头条面试的时候遇到一道题感觉很契合实际场景，唤起了我对线段树的记忆题目是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;给出一份服务器一天的log文件，包含用户id，登录时间，登出时间，求当天服务器最大在线人数是多少&lt;/p&gt;
&lt;/blockquote&gt;
&lt;
      
    
    </summary>
    
      <category term="Technology" scheme="http://yoursite.com/categories/Technology/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>笔记：细粒度级别图像分析：细粒度图像检索</title>
    <link href="http://yoursite.com/2019/05/10/%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%BB%86%E7%B2%92%E5%BA%A6%E7%BA%A7%E5%88%AB%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2019/05/10/笔记：细粒度级别图像分析/</id>
    <published>2019-05-10T02:48:53.000Z</published>
    <updated>2019-05-12T04:58:47.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="细粒度与粗粒度概念对比"><a href="#细粒度与粗粒度概念对比" class="headerlink" title="细粒度与粗粒度概念对比"></a>细粒度与粗粒度概念对比</h1><p><img src="/Users/billow/Desktop/1.png" alt=""></p><ul><li><p><strong>粗粒度</strong>：类间appearance差异较大，如猫、鸟、鱼、飞机、汽车等</p></li><li><p><strong>细粒度</strong>：类间差异较小，如不同品种的犬类之间，不同型号的飞机之间等</p></li></ul><h1 id="细粒度图像分类"><a href="#细粒度图像分类" class="headerlink" title="细粒度图像分类"></a>细粒度图像分类</h1><h2 id="细粒度分类难点"><a href="#细粒度分类难点" class="headerlink" title="细粒度分类难点"></a>细粒度分类难点</h2><ul><li><strong>apperance</strong>：类间差异较大（如猫和鸟），类内差异较小（如英短猫和美短猫）</li><li><strong>pose</strong>：类间差异可能较小（如不同品种，但相同姿态，相同视角布局），类内差异可能较大（如同一品种，但姿态、视角布局不同）</li></ul><p>需要同时兼顾类间和类内差异造成的影响</p><h2 id="细粒度图像分类关键"><a href="#细粒度图像分类关键" class="headerlink" title="细粒度图像分类关键"></a>细粒度图像分类关键</h2><p><img src="/Users/billow/Desktop/2.png" alt=""><br>找到分类对象的关键识别点</p><h2 id="Method1-SCDA"><a href="#Method1-SCDA" class="headerlink" title="Method1: SCDA"></a>Method1: SCDA</h2><h3 id="主要物体定位"><a href="#主要物体定位" class="headerlink" title="主要物体定位"></a>主要物体定位</h3><p>思路：用无监督的方法来定位主要物体，即用 pretrained 分类模型来获取特征图（如在 ImageNet 上训练的 VGG16）</p><blockquote><p>Notation</p><p><strong>Feature Map, Channel, Descriptor</strong></p><p><img src="/Users/billow/Desktop/3.png" alt=""><br>从网络中间拿到的 h <em> w </em> d 的tensor可以看成 h * w 个 d 维的 向量</p><p>也可以看成 d 个 h * w 的 channel</p><p>每个 d 维向量也称为一个深度描述子（deep descriptor）</p><p><strong>Distributed Representation</strong></p><p><img src="/Users/billow/Desktop/4.png" alt=""></p><p>每个 channel 会对物体不同部位进行响应，也有个别 channel 会对背景和噪声进行响应</p><p>Assumption：</p><p>绝大多数channel都会在主要物体上有所响应</p><p>因而很符合直觉地，我们可以从深度方向（d 方向）上对 channel 进行加和</p></blockquote><p><strong>一句话总结：对channel进行加和并设置阈值，过滤掉部分低响应区域，从而无监督地得到主要物体定位的 Mask</strong></p><p><img src="/Users/billow/Desktop/13.png" alt=""></p><h3 id="Feature-Aggregation"><a href="#Feature-Aggregation" class="headerlink" title="Feature Aggregation"></a>Feature Aggregation</h3><p>由于背景和噪声会对检索造成负面影响，所以我们利用主要物体定位的 Mask把用于描述主要物体的 descriptor 筛选出来，然后进行特征聚合作为最终检索的特征向量</p><blockquote><p>Notation</p><p><strong>VLAD, Fisher Vector, Pooling Approaches</strong></p><p>特征聚合常用的三种方法，其中 VLAD 是一阶方法，FV 是二阶</p><p><img src="/Users/billow/Desktop/5.png" alt=""></p><p>经过实验对比，单纯的池化结果就比 VLAD 和 FV 更好</p><p>推测是因为深度特征本身就具有很高的非线性，再进行一阶或二阶的特征聚合会导致过拟合，反而使得结果劣化</p></blockquote><p>我们将 avg pool 和 max pool 都结果进行级联后得到的特征称为 SCDA</p><h3 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h3><p>我们还可以对1024维的 SCDA 特征进行 Multi-layer ensumble，即提取网络不同深度的 tensor，以及图片水平翻转，进而得到 SCDA_flip 特征</p><blockquote><p>笔者去 Github 上搜了一下 SCDA 的代码，找到一份 keras 版本的挺清晰的，贴在这里帮助加深理解</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def select_aggregate(feat_map, resize=None):</span><br><span class="line">    A = tf.reduce_sum(feat_map, axis=-1, keepdims=True)</span><br><span class="line">    a = tf.reduce_mean(A, axis=[1, 2], keepdims=True)</span><br><span class="line">    M = tf.to_float(A&gt;a)</span><br><span class="line">    if resize != None:</span><br><span class="line">        M = tf.image.resize_images(M, resize)</span><br><span class="line">    return M</span><br><span class="line"></span><br><span class="line">def scda_plus(map1, map2, alpha=1.0):</span><br><span class="line">    _, h1, w1, _ = map1.shape</span><br><span class="line">    </span><br><span class="line">    M1 = select_aggregate(map1)</span><br><span class="line">    M2 = select_aggregate(map2)</span><br><span class="line"></span><br><span class="line">    S2 = map2 * M2</span><br><span class="line">    pavg2 = 1.0 / tf.reduce_sum(M2, axis=[1, 2]) * tf.reduce_sum(S2, axis=[1, 2]) # (b, d)</span><br><span class="line">    pmax2 = tf.reduce_max(S2, axis=[1, 2]) # (b, d)</span><br><span class="line">    S2 = tf.concat([pavg2, pmax2], axis=-1) # (b, 2d)</span><br><span class="line"></span><br><span class="line">    # upsampling</span><br><span class="line">    M2 = tf.image.resize_images(M2, [h1, w1], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</span><br><span class="line">    S1 = map1 * (M1 * M2)</span><br><span class="line">    pavg1 = 1.0 / tf.reduce_sum(M1, axis=[1, 2]) * tf.reduce_sum(S1, axis=[1, 2])</span><br><span class="line">    pmax1 = tf.reduce_max(S1, axis=[1, 2]) </span><br><span class="line">    S1 = tf.concat([pavg1, pmax1], axis=-1)</span><br><span class="line"></span><br><span class="line">    Splus = tf.concat([S2, S1*alpha], axis=-1) # (b, 4d)</span><br><span class="line">    Splus = tf.nn.l2_normalize(Splus, 0)</span><br><span class="line"></span><br><span class="line">    return Splus</span><br></pre></td></tr></table></figure><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p><img src="/Users/billow/Desktop/12.png" alt=""></p><p>对4096维的 SCDA 特征进行 SVD+whitening 处理后性能还会有进一步的提升</p><h2 id="Method2-DDT"><a href="#Method2-DDT" class="headerlink" title="Method2: DDT"></a>Method2: DDT</h2><h3 id="Image-Co-localization"><a href="#Image-Co-localization" class="headerlink" title="Image Co-localization"></a>Image Co-localization</h3><p>利用一个包含有某个共同物体的数据集，来对数据集中所有该物体进行定位。不需要知道这个物体的位置监督信息，甚至不需要知道这个物体具体是什么。</p><h3 id="DDT（Deep-Descriptor-Trasforming）"><a href="#DDT（Deep-Descriptor-Trasforming）" class="headerlink" title="DDT（Deep Descriptor Trasforming）"></a>DDT（Deep Descriptor Trasforming）</h3><p><img src="/Users/billow/Desktop/6.png" alt=""></p><p>利用 pretrained 网络对数据集所有图片提取tensor（每个 h <em> w </em> d 的tensor 都由 h * w 个深度描述子组成）</p><p>由于已知该数据集包含有某个共同物体，那么我们只需要衡量这 N <em> h </em> w 个 descriptor 之间的相关程度就可以了</p><p>因而我们只需要找到一个 mapping function，来把所有的 descriptor 映射到一个新的空间进行比较，在这个新空间中强相关的 descriptor 就应该是整个数据集中共同包含的物体对象所对应的描述子</p><p>PCA 技术可以很简单地做到这一点：PCA 实际上就是把一组 n 维向量映射到一组全新的 k 维正交向量上，所以我们可以把这些 d 维的深度描述子利用 PCA 降到1维，即只保留<strong>共同</strong>相关度最强的一个维度，<strong>在这个维度上的值就可以看做是该 descriptor 跟目标物体的相关程度</strong></p><p>因此，我们可以利用这个构建好的 PCA 来将每张图片的 tensor，转化为一个 h * w 的矩阵</p><p><img src="/Users/billow/Desktop/8.png" alt=""></p><p>利用该方法，我们也可以轻松地找出数据集中一些噪声</p><blockquote><p>笔者感觉 DDT 方法比 SCDA 更容易理解，Github 上也只搜到一个 caffe 版本的代码，就不贴出来了。个人感觉 DDT 的贡献主要是在定位方面，特征聚合和增强的方法仍可以参考 SCDA 的代码。</p></blockquote><h2 id="SCDA-vs-DDT"><a href="#SCDA-vs-DDT" class="headerlink" title="SCDA vs. DDT"></a>SCDA vs. DDT</h2><p><img src="/Users/billow/Desktop/7.png" alt=""></p><p>通过对比可以发现，由于 SCDA 是基于单张图片的，所以很容易把一些与对象无关的内容也定位进去，而 DDT 由于得到了数据集中其他图片信息的协同定位，能更准确地定位对应的物体对象</p><p>DDT 的另一个优势在于省去了 SCDA 方法中的动态阈值计算，只需要以0来作为阈值</p><h3 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h3><p><img src="/Users/billow/Desktop/9.png" alt=""></p><h3 id="泛化性"><a href="#泛化性" class="headerlink" title="泛化性"></a>泛化性</h3><p>将不同无监督方法在6个 ImageNet 没有的类别数据上进行实验对比<br><img src="/Users/billow/Desktop/10.png" alt=""></p><h2 id="DDT-方法的扩展"><a href="#DDT-方法的扩展" class="headerlink" title="DDT 方法的扩展"></a>DDT 方法的扩展</h2><p><img src="/Users/billow/Desktop/11.png" alt=""><br>[Collins et al., ECCV 2018]</p><p>对 tensor 进行NMF（非负矩阵分解）可以把物体的 key part 定位出来（如清真寺的穹顶，房屋主体等）</p><h2 id="内容来源："><a href="#内容来源：" class="headerlink" title="内容来源："></a>内容来源：</h2><p><a href="https://www.bilibili.com/video/av43499197/" target="_blank" rel="noopener">旷视科技南京研究院负责人魏秀参：细粒度级别图像分析领域的现状与展望</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;细粒度与粗粒度概念对比&quot;&gt;&lt;a href=&quot;#细粒度与粗粒度概念对比&quot; class=&quot;headerlink&quot; title=&quot;细粒度与粗粒度概念对比&quot;&gt;&lt;/a&gt;细粒度与粗粒度概念对比&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/Users/billow/Desktop/1.
      
    
    </summary>
    
      <category term="Technology" scheme="http://yoursite.com/categories/Technology/"/>
    
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
      <category term="Image Retrieval" scheme="http://yoursite.com/tags/Image-Retrieval/"/>
    
      <category term="Fine-Grained Image Analysis" scheme="http://yoursite.com/tags/Fine-Grained-Image-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>FAISS文档阅读（一）：预处理与后期处理</title>
    <link href="http://yoursite.com/2019/05/10/%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E5%90%8E%E6%9C%9F%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2019/05/10/预处理与后期处理/</id>
    <published>2019-05-10T02:48:53.000Z</published>
    <updated>2019-05-10T02:49:09.291Z</updated>
    
    <content type="html"><![CDATA[<h1 id="预处理与后期处理"><a href="#预处理与后期处理" class="headerlink" title="预处理与后期处理"></a>预处理与后期处理</h1><p>预处理与后期处理用于重映射向量和id，把转换处理(transformations)应用到数据，然后用更好的索引来对搜索结果进行重新排序</p><h2 id="映射Faiss-ID"><a href="#映射Faiss-ID" class="headerlink" title="映射Faiss ID"></a>映射Faiss ID</h2><p>默认情况下，Faiss分配一组id序列给被添加的向量来建立索引。</p><p>一些<strong>Index</strong>类会执行<strong>add_with_ids</strong>方法，该方法除了提供这些向量本身以外，还会提供64-bit的向量id。在搜索时，该类会返回存储好的id，而不是原始向量。</p><h3 id="IndexIDMap"><a href="#IndexIDMap" class="headerlink" title="IndexIDMap"></a>IndexIDMap</h3><p>这个索引会将另一个索引封装在内部，并在添加和搜索时对id进行转化。该索引维护了一张映射表。</p><p>举个例子：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">index = faiss.IndexFlatL2(xb.shape[1]) </span><br><span class="line">ids = np.arange(xb.shape[0])</span><br><span class="line">index.add_with_ids(xb, ids)  <span class="comment"># 这里会报错，因为IndexFlatL2不支持add_with_ids</span></span><br><span class="line">index2 = faiss.IndexIDMap(index)</span><br><span class="line">index2.add_with_ids(xb, ids) <span class="comment"># 这里可以正常运行，向量被存储在下一层的索引中</span></span><br></pre></td></tr></table></figure><h3 id="IndexIVF中的ID"><a href="#IndexIVF中的ID" class="headerlink" title="IndexIVF中的ID"></a>IndexIVF中的ID</h3><p><strong>IndexIVF</strong>子类总是存储向量ID，因此<strong>IndexIDMap</strong>的额外的映射表是浪费空间的。<strong>IndexIVF</strong>天生就提供了<strong>add_with_ids</strong></p><h2 id="预转换数据"><a href="#预转换数据" class="headerlink" title="预转换数据"></a>预转换数据</h2><p>该操作通常在转换数据比建立索引更优先的时候很有用。Transformation类继承<strong>VectorTransform</strong>。一个<strong>VectorTransform</strong>类对输入的<strong>d_in</strong>维向量应用转换方法，并输出大小为<strong>d_out</strong>的向量。</p><table><thead><tr><th style="text-align:center">转换方法</th><th style="text-align:center">类名称</th><th style="text-align:center">注释</th></tr></thead><tbody><tr><td style="text-align:center">随机旋转</td><td style="text-align:center"><strong>RandomRotationMatrix</strong></td><td style="text-align:center">在建立<strong>IndexPQ</strong>或<strong>IndexLSH</strong>索引之前有助于重新平衡向量组成</td><td></td></tr><tr><td style="text-align:center">维度重映射</td><td style="text-align:center"><strong>RemapDimensionsTransform</strong></td><td style="text-align:center">当索引方式有要求的维度，或要对各种维度应用一个随机的排列方式时，减少或增加向量尺寸</td><td></td></tr><tr><td style="text-align:center">PCA</td><td style="text-align:center"><strong>PCAMatrix</strong></td><td style="text-align:center">用于降维</td><td></td></tr><tr><td style="text-align:center">OPQ旋转</td><td style="text-align:center"><strong>OPQMatrix</strong></td><td style="text-align:center">OPQ会对输入的向量进行旋转来使其在PQ编码时更合理。详情查看Optimized product quantization, Ge et al., CVPR’13</td><td></td></tr></tbody></table><p>转换方法可以被一组向量训练，使用<strong>train</strong>方法，然后它们可以用<strong>apply</strong>方法来应用于另一组向量。</p><p>一种索引可以被封装于<strong>IndexPreTransform</strong>索引中来使得映射显式地进行，并使训练过程和索引训练综合进行。</p><h2 id="示例：使用PCA来降维"><a href="#示例：使用PCA来降维" class="headerlink" title="示例：使用PCA来降维"></a>示例：使用PCA来降维</h2><h3 id="配合IndexPreTransform"><a href="#配合IndexPreTransform" class="headerlink" title="配合IndexPreTransform"></a>配合IndexPreTransform</h3><p>如果输入向量是2048维的，且不得不减少到16 bytes，那么事先用PCA来降维就是合理的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> IndexIVFPQ索引应该设定为256维，而不是2048维</span></span><br><span class="line"> coarse_quantizer = faiss.IndexFlatL2 (256)</span><br><span class="line"> sub_index = faiss.IndexIVFPQ (coarse_quantizer, 256, ncoarse, 16, 8)</span><br><span class="line"><span class="meta"> #</span><span class="bash"> PCA 2048-&gt;256</span></span><br><span class="line"><span class="meta"> #</span><span class="bash"> 在降维后进行了随机旋转(第四个参数)</span></span><br><span class="line"> pca_matrix = faiss.PCAMatrix (2048, 256, 0, True) </span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash">- 封装索引</span></span><br><span class="line"> index = faiss.IndexPreTransform (pca_matrix, sub_index)</span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash"> 也需要对PCA进行训练</span></span><br><span class="line"> index.train(...)</span><br><span class="line"><span class="meta"> #</span><span class="bash"> PCA要比添加操作更早执行</span></span><br><span class="line"> index.add(...)</span><br></pre></td></tr></table></figure><h3 id="示例：升高维数"><a href="#示例：升高维数" class="headerlink" title="示例：升高维数"></a>示例：升高维数</h3><p>在往向量中插入0的时候，升高维数有时会很有用，这种方法对以下内容有帮助：</p><ul><li>让维数d增大4倍</li><li>让维数d增大M倍</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;预处理与后期处理&quot;&gt;&lt;a href=&quot;#预处理与后期处理&quot; class=&quot;headerlink&quot; title=&quot;预处理与后期处理&quot;&gt;&lt;/a&gt;预处理与后期处理&lt;/h1&gt;&lt;p&gt;预处理与后期处理用于重映射向量和id，把转换处理(transformations)应用到数据
      
    
    </summary>
    
      <category term="Technology" scheme="http://yoursite.com/categories/Technology/"/>
    
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
      <category term="Image Retrieval" scheme="http://yoursite.com/tags/Image-Retrieval/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="FAISS" scheme="http://yoursite.com/tags/FAISS/"/>
    
  </entry>
  
  <entry>
    <title>带有返回值的多线程Python实例</title>
    <link href="http://yoursite.com/2018/07/14/threads/"/>
    <id>http://yoursite.com/2018/07/14/threads/</id>
    <published>2018-07-14T06:02:14.000Z</published>
    <updated>2018-07-14T18:42:17.913Z</updated>
    
    <content type="html"><![CDATA[<h2 id="计算密集型-vs-IO密集型"><a href="#计算密集型-vs-IO密集型" class="headerlink" title="计算密集型 vs. IO密集型"></a>计算密集型 vs. IO密集型</h2><p>我们根据任务对CPU的使用情况，可以把任务类型分为计算密集型和IO密集型，计算密集型任务需要大量占用计算资源，适合使用多进程执行，每个进程可以使用独立的CPU核心进行计算；IO密集型任务需要的计算量不大，但可能存在大量的IO、网络延迟等待，适合使用多线程执行。</p><h2 id="有返回值的多线程实例"><a href="#有返回值的多线程实例" class="headerlink" title="有返回值的多线程实例"></a>有返回值的多线程实例</h2><p>Python初始的多线程对返回值的支持不是很好，我们可以通过类的继承来创建一个带有返回值的多线程类。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from threading import Thread</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span>(<span class="title">Thread</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, param1)</span></span><span class="symbol">:</span></span><br><span class="line">        Thread.__init_<span class="number">_</span>(<span class="keyword">self</span>)</span><br><span class="line">        <span class="keyword">self</span>.param1 = param1</span><br><span class="line">        <span class="keyword">self</span>.result = None</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(<span class="keyword">self</span>, param1)</span></span><span class="symbol">:</span></span><br><span class="line">        result = None</span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.result = <span class="keyword">self</span>.foo(<span class="keyword">self</span>.param1)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_result</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">self</span>.result</span><br></pre></td></tr></table></figure><p>然后我们就可以轻松地创建多线程：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    t = MyThread(i)</span><br><span class="line">    t_list.<span class="built_in">append</span>(t)</span><br><span class="line">    t.start()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> t_list:</span><br><span class="line">t.<span class="built_in">join</span>()</span><br><span class="line"><span class="built_in">print</span>(t.get_result())</span><br></pre></td></tr></table></figure><p>启动之后<strong>一定要t.join()</strong>将主程序阻塞，等待子进程运行结束，不然主线程比子线程跑的快，会拿不到结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;计算密集型-vs-IO密集型&quot;&gt;&lt;a href=&quot;#计算密集型-vs-IO密集型&quot; class=&quot;headerlink&quot; title=&quot;计算密集型 vs. IO密集型&quot;&gt;&lt;/a&gt;计算密集型 vs. IO密集型&lt;/h2&gt;&lt;p&gt;我们根据任务对CPU的使用情况，可以把任
      
    
    </summary>
    
      <category term="Technology" scheme="http://yoursite.com/categories/Technology/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Thread" scheme="http://yoursite.com/tags/Thread/"/>
    
  </entry>
  
  <entry>
    <title>Multiprocessing实用技能总结</title>
    <link href="http://yoursite.com/2018/07/13/multiprocessing/"/>
    <id>http://yoursite.com/2018/07/13/multiprocessing/</id>
    <published>2018-07-13T05:24:15.000Z</published>
    <updated>2018-07-13T18:07:30.409Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么要使用Python多进程"><a href="#为什么要使用Python多进程" class="headerlink" title="为什么要使用Python多进程"></a>为什么要使用Python多进程</h2><p>多线程并发具有内存共享、通信简单等优势，那么为什么我们还要选择多进程呢？下面我引用廖雪峰的Python教程中的一段说明：</p><blockquote><p>因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。</p></blockquote><blockquote><p>GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。</p></blockquote><blockquote><p>所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。</p></blockquote><blockquote><p>不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。</p></blockquote><h2 id="多进程的优势"><a href="#多进程的优势" class="headerlink" title="多进程的优势"></a>多进程的优势</h2><p>我们有很多的任务是相互独立运行的，也有很多任务实际上是可以通过拆分之后并行加速的。比如一个很大的list需要挨个遍历处理其中的元素，比如一组数据需要分别访问不同的方法处理得到不同的结果等等。</p><p>如果使用for loop来调用执行，实际上是一种串行的方式，必须要等待上一个任务结束才能执行下一个，中间会有大量的不必要的等待时间。</p><h2 id="Pool"><a href="#Pool" class="headerlink" title="Pool"></a>Pool</h2><p>我们使用进程池multiprocessing.Pool()来自动管理进程任务，首先要初始化进程池。</p><p>Pool方法接收一个参数MAX_TASKS，用于<strong>设置该进程池最多允许多少个子进程同时执行</strong>，若进程池中的子进程数大于MAX_TASKS，则剩余的子进程会进入等待，每当有子进程执行完毕，就会有新的子进程启动补充进去：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing import Pool</span><br><span class="line">MAX_TASKS = 5</span><br><span class="line">pool = Pool(MAX_TASKS)</span><br></pre></td></tr></table></figure><p>使用多进程可以加速计算的根本原因在于，可以利用多核CPU来分别执行不同的任务，因此最好先了解自己的CPU有多少个核心：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该语句可以获取自己CPU的核心数</span></span><br><span class="line"><span class="attr">NUM_CPUS</span> = multiprocessing.cpu_count()</span><br></pre></td></tr></table></figure><p><strong>为避免多余的任务切换开销，一般MAX_TASKS不要大于NUM_CPUS。</strong></p><p>然后我们需要定义一个function，该方法会被分配给每个新进程然后执行：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">child_task</span><span class="params">(param1, param2)</span></span><span class="symbol">:</span></span><br><span class="line"><span class="comment"># do something</span></span><br><span class="line"><span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>然后我们可以在主进程中调用apply_async方法来创建新进程并加入进程池中。</p><p>apply_async方法接受两个参数，第一个是要调用执行的方法，第二个是传递给该方法的参数：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">NUM_TASKS = <span class="number">10</span></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(NUM_TASKS):</span><br><span class="line">r = pool.apply_async(child_task, <span class="built_in">args</span>=(param1, param2))</span><br><span class="line">results.<span class="built_in">append</span>(r)</span><br><span class="line"></span><br><span class="line">p.<span class="built_in">close</span>() # 进程池停止接收新进程</span><br><span class="line">p.<span class="built_in">join</span>()  # 阻塞主进程，等待所有子进程结束</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> results:</span><br><span class="line"><span class="built_in">print</span>(r.<span class="built_in">get</span>())</span><br></pre></td></tr></table></figure><p>需要注意到上面的代码中，r.get()需要放到p.close()和p.join()执行进程池回收之后再使用。这是因为apply_async之后的语句是阻塞的，r.get()会等待上一条语句执行完毕才执行，因此<strong>获取返回值的过程最好放在进程池回收以后</strong>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;为什么要使用Python多进程&quot;&gt;&lt;a href=&quot;#为什么要使用Python多进程&quot; class=&quot;headerlink&quot; title=&quot;为什么要使用Python多进程&quot;&gt;&lt;/a&gt;为什么要使用Python多进程&lt;/h2&gt;&lt;p&gt;多线程并发具有内存共享、通信简单等优
      
    
    </summary>
    
      <category term="Technology" scheme="http://yoursite.com/categories/Technology/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="multiprocessing" scheme="http://yoursite.com/tags/multiprocessing/"/>
    
  </entry>
  
  <entry>
    <title>用FPN和ROIalign来进行图像检索</title>
    <link href="http://yoursite.com/2018/06/28/2018-6-28/"/>
    <id>http://yoursite.com/2018/06/28/2018-6-28/</id>
    <published>2018-06-28T03:21:40.000Z</published>
    <updated>2018-07-06T03:32:23.050Z</updated>
    
    <content type="html"><![CDATA[<p>上一版的模型出来以后测试发现一个问题，同一张图，低分辨率和高分辨率，检索结果完全不同。并且最气的是，这两张图相互查不到对方，模型输出的两个向量距离很远，这是不可接受的。</p><p>这个问题其实不难想到，深度模型每一层卷积都会对特征图进行压缩，在模型内部自然地形成一个金字塔形的结构（哪怕有padding操作，其实也只是在维持特征图尺寸，实际的信息依然被压缩，这是不可避免的）。就像一个逐渐抽象的过程中，势必要丢掉一些冗余信息，才能抓取到代表性的特征。</p><p>于是乎，当输入的图片本身分辨率很低，即含有的信息量很少时，这种深层压缩的结果会是毁灭性的。可以想象成从飞机上俯视地面上的一个人，你看到的或许仅仅是一个肉色的小色块了。</p><p>这个问题在目标检测中同样存在，之前流行的主流算法都是直接使用骨干网络的最后一层卷积图作为信息的载体，因此，当目标在图中的比例非常小时，往往很难被算法检测到。针对这个问题，Kaiming He大神提出了一个很厉害的通用结构FPN，仅仅是把它直接结合到Faster RCNN上，就让检测的召回率大大提升。</p><p>一般而言，不同阶段的卷积层输出含有不同层次的空间和语义信息，直接从模型的每一段提出来的卷积图是不能混合使用的。FPN通过top-down和横向连接，以及一个3*3的smooth layer解决了这个问题，使不同阶段的特征图语义统一成为现实。</p><p>对于低分辨率的图片，通过把深层卷积图上采样叠加到浅层卷积图上，可以在保留高层次语义信息的基础上，极大地保留空间信息。</p><blockquote><p>k = floor(4 + sqrt(w * h) / 224)</p></blockquote><p>通过这个公式，可以自动根据输入图片的尺寸来调整获取对应尺度的特征图。</p><p>这里有一个疑惑是，很明显在FPN结构中，P2会是该结构受益最大的层，因为它整合了所有阶段的卷积图信息，包含了最丰富的空间和语义信息。在Paper的消融实验里也证实，只使用P2卷积图就能得到非常好的效果。那么为什么还要使用不同阶段的特征图？或者说，何不直接对所有尺度的图片使用P2特征图？</p><p>我的猜测是，也许对于高分辨率图片，其包含的空间信息过于庞大，即使通过深层模型依然可以有所保留，因而不需要再从浅层卷积图获取，这种特征图叠加融合的手段，主要目的还是在改善低分辨率的性能，是一种补偿手段，在高分辨率图上反而会因信息过多而再次带来语义的不统一，这是一种空间与语义的权衡折中。</p><p>从直觉上，通过在图片上建立FPN，再选择合适的尺度特征，就可以用来进行图像检索。相较于目标检测中有RPN提供ROI，在图像检索中只能以整张图作为输入，与ROI类似的，图片的比例也往往不是标准的1:1，提取得到的特征图也是如此，直接插值放缩是不可行的，会破坏原有的空间结构，因此还需要引入ROI池化来使得特征维度统一。根据Mask RCNN的研究，ROI池化存在各种各样的不足之处，ROIalign是一个更好的策略。</p><p>图像检索不同于目标检测的另一点是，ROI的边界往往是紧贴检测对象的，换言之，目标会充满整个ROI；而图片并非如此，往往还会存在大量的留白区域，因此直接对全图进行ROIalign同样会导致得到的特征的不一致，这里我才用了以下策略来对要输入模型的图片进行预处理：</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 伪代码</span></span><br><span class="line"><span class="meta"># 获取长边</span></span><br><span class="line">pad = img.width <span class="keyword">if</span> img.width &gt; img.height <span class="keyword">else</span> img.height</span><br><span class="line"><span class="meta"># 用长边对图片进行扩充</span></span><br><span class="line">Padding(img)</span><br><span class="line"><span class="meta"># 从图片中心截取pad*pad的区域</span></span><br><span class="line">CenterCrop(img, pad)</span><br></pre></td></tr></table></figure><p>如此一来，我们就得到了将短边延拓到长边尺寸的图片。我们在这个图片的基础上进行FPN和ROIalign得到最终的特征向量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上一版的模型出来以后测试发现一个问题，同一张图，低分辨率和高分辨率，检索结果完全不同。并且最气的是，这两张图相互查不到对方，模型输出的两个向量距离很远，这是不可接受的。&lt;/p&gt;
&lt;p&gt;这个问题其实不难想到，深度模型每一层卷积都会对特征图进行压缩，在模型内部自然地形成一个金字
      
    
    </summary>
    
      <category term="Technology" scheme="http://yoursite.com/categories/Technology/"/>
    
    
      <category term="Image Retrieval" scheme="http://yoursite.com/tags/Image-Retrieval/"/>
    
      <category term="FPN" scheme="http://yoursite.com/tags/FPN/"/>
    
      <category term="ROIalign" scheme="http://yoursite.com/tags/ROIalign/"/>
    
  </entry>
  
  <entry>
    <title>随笔：飞机上1</title>
    <link href="http://yoursite.com/2018/05/28/2018-5-28/"/>
    <id>http://yoursite.com/2018/05/28/2018-5-28/</id>
    <published>2018-05-28T13:44:12.000Z</published>
    <updated>2020-05-07T07:09:30.611Z</updated>
    
    <content type="html"><![CDATA[<p>2018.5.28 21:00 成都-&gt;北京</p><p>在生活中你我应该都有过这样的经历：自己的某种想法、某种情感、某种感受，找不到任何词句来形容，不论怎么组织语言，表达出来的东西似乎都离自己内心的那个点差了些什么。</p><p>你我应该也都有过这种体验：某时某刻，突然听到某个人说的一句话，某首歌的一节词，某篇文章的一些句子，突然直戳内心，道尽了你一直以来难以言表的某些东西，使你感到从内到外的深切共鸣。</p><p>夏目漱石曾有一个经典的比喻，他形容日语是高情境的语言，当日本人希望表达“我爱你”时，用“月色真美”将会更加恰当。木心说“文字的简练来源于内心的真诚，我十二万分的爱你，就不如说，我爱你”。</p><p>我们的思想、情感、感受、思维，应该是一种很高维度的东西，而相比之下，我们的语言是低维的。所以才会出现那么多的词不达意、难以言说。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2018.5.28 21:00 成都-&amp;gt;北京&lt;/p&gt;
&lt;p&gt;在生活中你我应该都有过这样的经历：自己的某种想法、某种情感、某种感受，找不到任何词句来形容，不论怎么组织语言，表达出来的东西似乎都离自己内心的那个点差了些什么。&lt;/p&gt;
&lt;p&gt;你我应该也都有过这种体验：某时某
      
    
    </summary>
    
      <category term="Notes" scheme="http://yoursite.com/categories/Notes/"/>
    
    
      <category term="思维 语言 夏目漱石 木心 维度" scheme="http://yoursite.com/tags/%E6%80%9D%E7%BB%B4-%E8%AF%AD%E8%A8%80-%E5%A4%8F%E7%9B%AE%E6%BC%B1%E7%9F%B3-%E6%9C%A8%E5%BF%83-%E7%BB%B4%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：Faster R-CNN Features for Instance Search</title>
    <link href="http://yoursite.com/2018/05/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9AFaster%20R-CNN%20Features%20for%20Instance%20Search/"/>
    <id>http://yoursite.com/2018/05/07/论文阅读笔记：Faster R-CNN Features for Instance Search/</id>
    <published>2018-05-06T16:36:53.000Z</published>
    <updated>2018-05-09T09:25:55.192Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CVPR-2016-Faster-R-CNN-Features-for-Instance-Search"><a href="#CVPR-2016-Faster-R-CNN-Features-for-Instance-Search" class="headerlink" title="[CVPR 2016] Faster R-CNN Features for Instance Search"></a>[CVPR 2016] Faster R-CNN Features for Instance Search</h1><p>Paper: <a href="https://arxiv.org/abs/1604.08893" target="_blank" rel="noopener">Link</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>研究基于CNN的目标检测网络得到的feature是否有利于image retrieval</li><li>研究对目标检测网络进行fine-tune后检索效果是否有所提升</li><li>实验数据集：<ul><li>Oxford Building 5K</li><li>Paris Buildings 6K</li><li>TRECVid Instance Search 2013</li></ul></li></ul><h3 id="What-they-do"><a href="#What-they-do" class="headerlink" title="What they do"></a>What they do</h3><ul><li>通过一次前向传播从CNN中提取出图像的全局和局部特征</li><li>利用从RPN中学到的位置信息设计空间重排方案</li><li>分析了对目标检测CNN进行fine-tune对检索的影响</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>利用目标检测的CNN特征可以用于图像检索</li><li>使用检索的图像对CNN进行微调，可以较大提升检索效果</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ol><li>利用预训练的CNNs提取现成的特征，在图像检索通用数据集上获得了不错的效果</li><li>实例检索系统 = 快速筛选（对数据库进行重排） + 高级检索（一些高计算量的阶段）。</li><li>常见的重排方式有<strong>几何验证</strong>和<strong>空间分析</strong></li><li>空间分析重排是指，使用不同尺寸的滑动窗口滑过图像，获得图像内部不同局部区域的特征表达，将这些局部特征与查询实例进行比对。</li></ol><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="CNNs-for-Instance-Search"><a href="#CNNs-for-Instance-Search" class="headerlink" title="CNNs for Instance Search"></a>CNNs for Instance Search</h4><ul><li>早期研究中利用全连接层的特征进行图像检索</li><li>将从不同图像sub-patches提取的全连接层特征进行结合，得到更好的结果</li><li>研究发现图像检索领域使用卷积层特征进行，比全连接层特征结果更好</li></ul><h4 id="Object-Detection-CNNs"><a href="#Object-Detection-CNNs" class="headerlink" title="Object Detection CNNs"></a>Object Detection CNNs</h4><ul><li>早期使用滑动窗口方法</li><li>随后R-CNN采用Selective Search</li><li>现在使用端到端的方法</li></ul><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="CNN-based-Representations"><a href="#CNN-based-Representations" class="headerlink" title="CNN-based Representations"></a>CNN-based Representations</h3><ul><li>作者将查询实例定义为找到<strong>包含查询目标的边界框</strong></li><li>提出两种池化方案：<ul><li><strong>Image-wise pooling of activations(IPA)</strong>:忽略操作目标建议的所有层，只从最后一个卷积层提取特征，对所有滤波器的输出进行求和</li><li><strong>Region-wise pooling of activations(RPA)</strong>：利用区域池化层提取RPN得到的目标建议的特征。对于每个建议窗口，都可以利用其中RoI池化层的输出来生成表达。</li></ul></li><li>求和池化需要进行L2标准化、白化、L2标准化，而最大池化仅仅进行一次L2标准化</li></ul><h3 id="Fine-tuning-Faster-R-CNN"><a href="#Fine-tuning-Faster-R-CNN" class="headerlink" title="Fine-tuning Faster R-CNN"></a>Fine-tuning Faster R-CNN</h3><ul><li>作者提出了两种微调方案：<ol><li>只更新分类这一分支的全连接层的权重</li><li>前两个卷积层之后的所有层都更新</li></ol></li></ul><h3 id="Image-Retrieval"><a href="#Image-Retrieval" class="headerlink" title="Image Retrieval"></a>Image Retrieval</h3><ul><li>作者提出检索由三步构成：<ol><li>快速筛选</li><li>空间重排</li><li>扩展查询</li></ol></li><li>快速筛选是利用IPA，即图像的全局表达进行一次过滤，只保留前N个结果</li><li>空间重排由两种方法：<ol><li>CA-SR：假设类别不可知，将查询实例经过网络得到的所有RPA，与快速筛选后剩下的每幅图的RPA进行比较</li><li>CS-SR：使用微调后的网络，可以用每个RPN预测的分类得分作为查询对象的相似分数</li></ol></li><li>扩展查询策略为取重排后的前M个结果的特征向量，对他们进行求和平均，用平均后得到的结果重新进行一次查询</li></ul><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><p>图像检索实际上是把图片转为向量在高维空间中位置的比较，两个向量越近，距离越短，则说明越相似。直接使用从整张图片得到的特征向量进行比对的图片检索系统效果有很大的优化空间，因为图片中与检索目标无关的信息越多，就会加大向量在其他维度的权重，进而使得向量在空间中的位置发生偏移，与希望得到的结果的距离变大，这种现象随着查询目标在原图中所占比例的大小越小、显著性越低、对比越不强烈，检索偏差也会越大。</p><p>举一个比较简单而极端的例子：我们可以想象有一个10维的空间，在这个空间中，只有第1，2，3维的方向上代表了我们所查询的对象的表达（实际上可能每个维度上都有不同的权重，在这里我们极端地假设其他维度上权重为0）。通过CNN得到的特征向量，实际上是由图片内的内容共同组成的，与我们所查询的对象无关的内容也掺杂其中，这些无关内容在10维空间中标示为其他维度上的不同权重。</p><p>因此，图片全局的表达可以用作第一步的快速筛选，然后再使用局部表达精确检索。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CVPR-2016-Faster-R-CNN-Features-for-Instance-Search&quot;&gt;&lt;a href=&quot;#CVPR-2016-Faster-R-CNN-Features-for-Instance-Search&quot; class=&quot;headerlin
      
    
    </summary>
    
      <category term="Technology" scheme="http://yoursite.com/categories/Technology/"/>
    
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
      <category term="Image Retrieval" scheme="http://yoursite.com/tags/Image-Retrieval/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
