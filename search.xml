<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[线段树Python]]></title>
    <url>%2F2019%2F07%2F16%2F%E7%BA%BF%E6%AE%B5%E6%A0%91Python%2F</url>
    <content type="text"><![CDATA[最近在参加秋招，头条面试的时候遇到一道题感觉很契合实际场景，唤起了我对线段树的记忆题目是这样的： 给出一份服务器一天的log文件，包含用户id，登录时间，登出时间，求当天服务器最大在线人数是多少 当时看完这个题第一反应就是可以用线段树，但是线段树我以前只是了解了一下概念和适用场景，没有真正仔细研读，趁此机会算是好好学习了一下 线段树网上的教程一搜一大把，不过很少有Python版本的，能搜到的也都没有区间更新或者是维护节点和，而不是求最大值，所以把我的版本贴出来 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 定义树节点，l,r, val表示该节点记录的是区间[l, r]的最大值是valclass Tree(): def __init__(self): self.l = 0 self.r = 0 self.lazy = 0 self.val = 0 # 二叉树是堆形式，可以用一维数组存储，注意数组长度要开4倍空间tree = [Tree() for i in range(10*4)]# 建树，用cur&lt;&lt;1访问左子树，cur&lt;&lt;1|1访问右子树，位运算操作很方便def build(cur, l, r): tree[cur].l, tree[cur].r, tree[cur].lazy, tree[cur].val = l, r, 0, 0 # 当l==r的时候结束递归 if l &lt; r: mid = l + r &gt;&gt; 1 build(cur&lt;&lt;1, l, mid) build(cur&lt;&lt;1|1, mid+1, r)# 当子节点计算完成后，用子节点的值来更新自己的值def pushup(cur): tree[cur].val = max(tree[cur&lt;&lt;1].val, tree[cur&lt;&lt;1|1].val) # 单点更新def add(cur, x, v): if tree[cur].l == tree[cur].r: tree[cur].val += v else: mid = tree[cur].r + tree[cur].l &gt;&gt; 1 if x &gt; mid: add(cur&gt;&gt;1|1, x, v) else: add(cur&lt;&lt;1, x, v) pushup(cur)# 将lazy标记向下传递一层def pushdown(cur): if tree[cur].lazy: lazy = tree[cur].lazy tree[cur&lt;&lt;1].lazy += lazy tree[cur&lt;&lt;1|1].lazy += lazy tree[cur&lt;&lt;1].val += lazy tree[cur&lt;&lt;1|1].val += lazy tree[cur].lazy = 0# 区间更新def update(cur, l, r, v): if l &lt;= tree[cur].l and tree[cur].r &lt;= r: tree[cur].lazy += v tree[cur].val += v return if r &lt; tree[cur].l or l &gt; tree[cur].r: return if tree[cur].lazy: pushdown(cur) update(cur&lt;&lt;1, l, r, v) update(cur&lt;&lt;1|1, l, r, v) pushup(cur)# 区间查询def query(cur, l, r): if tree[cur].l &lt;= l and r &lt;= tree[cur].r: return tree[cur].val if tree[cur].l &gt; r or tree[cur].r &lt; l: return 0 if tree[cur].lazy: pushdown(cur) return max(query(cur&lt;&lt;1, l, r), query(cur&lt;&lt;1|1))# 测试# -----# ---# -------# --# --build(1, 1, 10)update(1, 1, 5, 1)update(1, 7, 10, 1)update(1, 2, 8, 1)update(1, 3, 4, 1)update(1, 9, 10, 1)print(query(1, 1, 10))# 返回为3]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记：细粒度级别图像分析：细粒度图像检索]]></title>
    <url>%2F2019%2F05%2F10%2F%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%BB%86%E7%B2%92%E5%BA%A6%E7%BA%A7%E5%88%AB%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[细粒度与粗粒度概念对比 粗粒度：类间appearance差异较大，如猫、鸟、鱼、飞机、汽车等 细粒度：类间差异较小，如不同品种的犬类之间，不同型号的飞机之间等 细粒度图像分类细粒度分类难点 apperance：类间差异较大（如猫和鸟），类内差异较小（如英短猫和美短猫） pose：类间差异可能较小（如不同品种，但相同姿态，相同视角布局），类内差异可能较大（如同一品种，但姿态、视角布局不同） 需要同时兼顾类间和类内差异造成的影响 细粒度图像分类关键找到分类对象的关键识别点 Method1: SCDA主要物体定位思路：用无监督的方法来定位主要物体，即用 pretrained 分类模型来获取特征图（如在 ImageNet 上训练的 VGG16） Notation Feature Map, Channel, Descriptor 从网络中间拿到的 h w d 的tensor可以看成 h * w 个 d 维的 向量 也可以看成 d 个 h * w 的 channel 每个 d 维向量也称为一个深度描述子（deep descriptor） Distributed Representation 每个 channel 会对物体不同部位进行响应，也有个别 channel 会对背景和噪声进行响应 Assumption： 绝大多数channel都会在主要物体上有所响应 因而很符合直觉地，我们可以从深度方向（d 方向）上对 channel 进行加和 一句话总结：对channel进行加和并设置阈值，过滤掉部分低响应区域，从而无监督地得到主要物体定位的 Mask Feature Aggregation由于背景和噪声会对检索造成负面影响，所以我们利用主要物体定位的 Mask把用于描述主要物体的 descriptor 筛选出来，然后进行特征聚合作为最终检索的特征向量 Notation VLAD, Fisher Vector, Pooling Approaches 特征聚合常用的三种方法，其中 VLAD 是一阶方法，FV 是二阶 经过实验对比，单纯的池化结果就比 VLAD 和 FV 更好 推测是因为深度特征本身就具有很高的非线性，再进行一阶或二阶的特征聚合会导致过拟合，反而使得结果劣化 我们将 avg pool 和 max pool 都结果进行级联后得到的特征称为 SCDA 后处理我们还可以对1024维的 SCDA 特征进行 Multi-layer ensumble，即提取网络不同深度的 tensor，以及图片水平翻转，进而得到 SCDA_flip 特征 笔者去 Github 上搜了一下 SCDA 的代码，找到一份 keras 版本的挺清晰的，贴在这里帮助加深理解 123456789101112131415161718192021222324252627282930def select_aggregate(feat_map, resize=None): A = tf.reduce_sum(feat_map, axis=-1, keepdims=True) a = tf.reduce_mean(A, axis=[1, 2], keepdims=True) M = tf.to_float(A&gt;a) if resize != None: M = tf.image.resize_images(M, resize) return Mdef scda_plus(map1, map2, alpha=1.0): _, h1, w1, _ = map1.shape M1 = select_aggregate(map1) M2 = select_aggregate(map2) S2 = map2 * M2 pavg2 = 1.0 / tf.reduce_sum(M2, axis=[1, 2]) * tf.reduce_sum(S2, axis=[1, 2]) # (b, d) pmax2 = tf.reduce_max(S2, axis=[1, 2]) # (b, d) S2 = tf.concat([pavg2, pmax2], axis=-1) # (b, 2d) # upsampling M2 = tf.image.resize_images(M2, [h1, w1], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) S1 = map1 * (M1 * M2) pavg1 = 1.0 / tf.reduce_sum(M1, axis=[1, 2]) * tf.reduce_sum(S1, axis=[1, 2]) pmax1 = tf.reduce_max(S1, axis=[1, 2]) S1 = tf.concat([pavg1, pmax1], axis=-1) Splus = tf.concat([S2, S1*alpha], axis=-1) # (b, 4d) Splus = tf.nn.l2_normalize(Splus, 0) return Splus 结果分析 对4096维的 SCDA 特征进行 SVD+whitening 处理后性能还会有进一步的提升 Method2: DDTImage Co-localization利用一个包含有某个共同物体的数据集，来对数据集中所有该物体进行定位。不需要知道这个物体的位置监督信息，甚至不需要知道这个物体具体是什么。 DDT（Deep Descriptor Trasforming） 利用 pretrained 网络对数据集所有图片提取tensor（每个 h w d 的tensor 都由 h * w 个深度描述子组成） 由于已知该数据集包含有某个共同物体，那么我们只需要衡量这 N h w 个 descriptor 之间的相关程度就可以了 因而我们只需要找到一个 mapping function，来把所有的 descriptor 映射到一个新的空间进行比较，在这个新空间中强相关的 descriptor 就应该是整个数据集中共同包含的物体对象所对应的描述子 PCA 技术可以很简单地做到这一点：PCA 实际上就是把一组 n 维向量映射到一组全新的 k 维正交向量上，所以我们可以把这些 d 维的深度描述子利用 PCA 降到1维，即只保留共同相关度最强的一个维度，在这个维度上的值就可以看做是该 descriptor 跟目标物体的相关程度 因此，我们可以利用这个构建好的 PCA 来将每张图片的 tensor，转化为一个 h * w 的矩阵 利用该方法，我们也可以轻松地找出数据集中一些噪声 笔者感觉 DDT 方法比 SCDA 更容易理解，Github 上也只搜到一个 caffe 版本的代码，就不贴出来了。个人感觉 DDT 的贡献主要是在定位方面，特征聚合和增强的方法仍可以参考 SCDA 的代码。 SCDA vs. DDT 通过对比可以发现，由于 SCDA 是基于单张图片的，所以很容易把一些与对象无关的内容也定位进去，而 DDT 由于得到了数据集中其他图片信息的协同定位，能更准确地定位对应的物体对象 DDT 的另一个优势在于省去了 SCDA 方法中的动态阈值计算，只需要以0来作为阈值 Benchmark 泛化性将不同无监督方法在6个 ImageNet 没有的类别数据上进行实验对比 DDT 方法的扩展[Collins et al., ECCV 2018] 对 tensor 进行NMF（非负矩阵分解）可以把物体的 key part 定位出来（如清真寺的穹顶，房屋主体等） 内容来源：旷视科技南京研究院负责人魏秀参：细粒度级别图像分析领域的现状与展望]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Image Retrieval</tag>
        <tag>CV</tag>
        <tag>Fine-Grained Image Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FAISS文档阅读（一）：预处理与后期处理]]></title>
    <url>%2F2019%2F05%2F10%2F%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E5%90%8E%E6%9C%9F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[预处理与后期处理预处理与后期处理用于重映射向量和id，把转换处理(transformations)应用到数据，然后用更好的索引来对搜索结果进行重新排序 映射Faiss ID默认情况下，Faiss分配一组id序列给被添加的向量来建立索引。 一些Index类会执行add_with_ids方法，该方法除了提供这些向量本身以外，还会提供64-bit的向量id。在搜索时，该类会返回存储好的id，而不是原始向量。 IndexIDMap这个索引会将另一个索引封装在内部，并在添加和搜索时对id进行转化。该索引维护了一张映射表。 举个例子： 12345index = faiss.IndexFlatL2(xb.shape[1]) ids = np.arange(xb.shape[0])index.add_with_ids(xb, ids) # 这里会报错，因为IndexFlatL2不支持add_with_idsindex2 = faiss.IndexIDMap(index)index2.add_with_ids(xb, ids) # 这里可以正常运行，向量被存储在下一层的索引中 IndexIVF中的IDIndexIVF子类总是存储向量ID，因此IndexIDMap的额外的映射表是浪费空间的。IndexIVF天生就提供了add_with_ids 预转换数据该操作通常在转换数据比建立索引更优先的时候很有用。Transformation类继承VectorTransform。一个VectorTransform类对输入的d_in维向量应用转换方法，并输出大小为d_out的向量。 转换方法 类名称 注释 随机旋转 RandomRotationMatrix 在建立IndexPQ或IndexLSH索引之前有助于重新平衡向量组成 维度重映射 RemapDimensionsTransform 当索引方式有要求的维度，或要对各种维度应用一个随机的排列方式时，减少或增加向量尺寸 PCA PCAMatrix 用于降维 OPQ旋转 OPQMatrix OPQ会对输入的向量进行旋转来使其在PQ编码时更合理。详情查看Optimized product quantization, Ge et al., CVPR’13 转换方法可以被一组向量训练，使用train方法，然后它们可以用apply方法来应用于另一组向量。 一种索引可以被封装于IndexPreTransform索引中来使得映射显式地进行，并使训练过程和索引训练综合进行。 示例：使用PCA来降维配合IndexPreTransform如果输入向量是2048维的，且不得不减少到16 bytes，那么事先用PCA来降维就是合理的。 1234567891011121314# IndexIVFPQ索引应该设定为256维，而不是2048维 coarse_quantizer = faiss.IndexFlatL2 (256) sub_index = faiss.IndexIVFPQ (coarse_quantizer, 256, ncoarse, 16, 8) # PCA 2048-&gt;256 # 在降维后进行了随机旋转(第四个参数) pca_matrix = faiss.PCAMatrix (2048, 256, 0, True) #- 封装索引 index = faiss.IndexPreTransform (pca_matrix, sub_index) # 也需要对PCA进行训练 index.train(...) # PCA要比添加操作更早执行 index.add(...) 示例：升高维数在往向量中插入0的时候，升高维数有时会很有用，这种方法对以下内容有帮助： 让维数d增大4倍 让维数d增大M倍]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Image Retrieval</tag>
        <tag>CV</tag>
        <tag>Deep Learning</tag>
        <tag>FAISS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带有返回值的多线程Python实例]]></title>
    <url>%2F2018%2F07%2F14%2Fthreads%2F</url>
    <content type="text"><![CDATA[计算密集型 vs. IO密集型我们根据任务对CPU的使用情况，可以把任务类型分为计算密集型和IO密集型，计算密集型任务需要大量占用计算资源，适合使用多进程执行，每个进程可以使用独立的CPU核心进行计算；IO密集型任务需要的计算量不大，但可能存在大量的IO、网络延迟等待，适合使用多线程执行。 有返回值的多线程实例Python初始的多线程对返回值的支持不是很好，我们可以通过类的继承来创建一个带有返回值的多线程类。 12345678910111213141516171819from threading import Thread class MyThread(Thread): def __init__(self, param1): Thread.__init__(self) self.param1 = param1 self.result = None def foo(self, param1): result = None # do something return result def run(self): self.result = self.foo(self.param1) def get_result(self): return self.result 然后我们就可以轻松地创建多线程： 123456789t_list = []for i in range(3): t = MyThread(i) t_list.append(t) t.start() for t in t_list: t.join() print(t.get_result()) 启动之后一定要t.join()将主程序阻塞，等待子进程运行结束，不然主线程比子线程跑的快，会拿不到结果。]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Multiprocessing实用技能总结]]></title>
    <url>%2F2018%2F07%2F13%2Fmultiprocessing%2F</url>
    <content type="text"><![CDATA[为什么要使用Python多进程多线程并发具有内存共享、通信简单等优势，那么为什么我们还要选择多进程呢？下面我引用廖雪峰的Python教程中的一段说明： 因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。 GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。 所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。 不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 多进程的优势我们有很多的任务是相互独立运行的，也有很多任务实际上是可以通过拆分之后并行加速的。比如一个很大的list需要挨个遍历处理其中的元素，比如一组数据需要分别访问不同的方法处理得到不同的结果等等。 如果使用for loop来调用执行，实际上是一种串行的方式，必须要等待上一个任务结束才能执行下一个，中间会有大量的不必要的等待时间。 Pool我们使用进程池multiprocessing.Pool()来自动管理进程任务，首先要初始化进程池。 Pool方法接收一个参数MAX_TASKS，用于设置该进程池最多允许多少个子进程同时执行，若进程池中的子进程数大于MAX_TASKS，则剩余的子进程会进入等待，每当有子进程执行完毕，就会有新的子进程启动补充进去： 123from multiprocessing import PoolMAX_TASKS = 5pool = Pool(MAX_TASKS) 使用多进程可以加速计算的根本原因在于，可以利用多核CPU来分别执行不同的任务，因此最好先了解自己的CPU有多少个核心： 12# 该语句可以获取自己CPU的核心数NUM_CPUS = multiprocessing.cpu_count() 为避免多余的任务切换开销，一般MAX_TASKS不要大于NUM_CPUS。 然后我们需要定义一个function，该方法会被分配给每个新进程然后执行： 123def child_task(param1, param2): # do something return result 然后我们可以在主进程中调用apply_async方法来创建新进程并加入进程池中。 apply_async方法接受两个参数，第一个是要调用执行的方法，第二个是传递给该方法的参数： 1234567891011NUM_TASKS = 10results = []for i in range(NUM_TASKS): r = pool.apply_async(child_task, args=(param1, param2)) results.append(r)p.close() # 进程池停止接收新进程p.join() # 阻塞主进程，等待所有子进程结束for r in results: print(r.get()) 需要注意到上面的代码中，r.get()需要放到p.close()和p.join()执行进程池回收之后再使用。这是因为apply_async之后的语句是阻塞的，r.get()会等待上一条语句执行完毕才执行，因此获取返回值的过程最好放在进程池回收以后。]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>multiprocessing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用FPN和ROIalign来进行图像检索]]></title>
    <url>%2F2018%2F06%2F28%2F2018-6-28%2F</url>
    <content type="text"><![CDATA[上一版的模型出来以后测试发现一个问题，同一张图，低分辨率和高分辨率，检索结果完全不同。并且最气的是，这两张图相互查不到对方，模型输出的两个向量距离很远，这是不可接受的。 这个问题其实不难想到，深度模型每一层卷积都会对特征图进行压缩，在模型内部自然地形成一个金字塔形的结构（哪怕有padding操作，其实也只是在维持特征图尺寸，实际的信息依然被压缩，这是不可避免的）。就像一个逐渐抽象的过程中，势必要丢掉一些冗余信息，才能抓取到代表性的特征。 于是乎，当输入的图片本身分辨率很低，即含有的信息量很少时，这种深层压缩的结果会是毁灭性的。可以想象成从飞机上俯视地面上的一个人，你看到的或许仅仅是一个肉色的小色块了。 这个问题在目标检测中同样存在，之前流行的主流算法都是直接使用骨干网络的最后一层卷积图作为信息的载体，因此，当目标在图中的比例非常小时，往往很难被算法检测到。针对这个问题，Kaiming He大神提出了一个很厉害的通用结构FPN，仅仅是把它直接结合到Faster RCNN上，就让检测的召回率大大提升。 一般而言，不同阶段的卷积层输出含有不同层次的空间和语义信息，直接从模型的每一段提出来的卷积图是不能混合使用的。FPN通过top-down和横向连接，以及一个3*3的smooth layer解决了这个问题，使不同阶段的特征图语义统一成为现实。 对于低分辨率的图片，通过把深层卷积图上采样叠加到浅层卷积图上，可以在保留高层次语义信息的基础上，极大地保留空间信息。 k = floor(4 + sqrt(w * h) / 224) 通过这个公式，可以自动根据输入图片的尺寸来调整获取对应尺度的特征图。 这里有一个疑惑是，很明显在FPN结构中，P2会是该结构受益最大的层，因为它整合了所有阶段的卷积图信息，包含了最丰富的空间和语义信息。在Paper的消融实验里也证实，只使用P2卷积图就能得到非常好的效果。那么为什么还要使用不同阶段的特征图？或者说，何不直接对所有尺度的图片使用P2特征图？ 我的猜测是，也许对于高分辨率图片，其包含的空间信息过于庞大，即使通过深层模型依然可以有所保留，因而不需要再从浅层卷积图获取，这种特征图叠加融合的手段，主要目的还是在改善低分辨率的性能，是一种补偿手段，在高分辨率图上反而会因信息过多而再次带来语义的不统一，这是一种空间与语义的权衡折中。 从直觉上，通过在图片上建立FPN，再选择合适的尺度特征，就可以用来进行图像检索。相较于目标检测中有RPN提供ROI，在图像检索中只能以整张图作为输入，与ROI类似的，图片的比例也往往不是标准的1:1，提取得到的特征图也是如此，直接插值放缩是不可行的，会破坏原有的空间结构，因此还需要引入ROI池化来使得特征维度统一。根据Mask RCNN的研究，ROI池化存在各种各样的不足之处，ROIalign是一个更好的策略。 图像检索不同于目标检测的另一点是，ROI的边界往往是紧贴检测对象的，换言之，目标会充满整个ROI；而图片并非如此，往往还会存在大量的留白区域，因此直接对全图进行ROIalign同样会导致得到的特征的不一致，这里我才用了以下策略来对要输入模型的图片进行预处理： 1234567# 伪代码# 获取长边pad = img.width if img.width &gt; img.height else img.height# 用长边对图片进行扩充Padding(img)# 从图片中心截取pad*pad的区域CenterCrop(img, pad) 如此一来，我们就得到了将短边延拓到长边尺寸的图片。我们在这个图片的基础上进行FPN和ROIalign得到最终的特征向量。]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>FPN</tag>
        <tag>ROIalign</tag>
        <tag>Image Retrieval</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随笔：飞机上1]]></title>
    <url>%2F2018%2F05%2F28%2F2018-5-28%2F</url>
    <content type="text"><![CDATA[2018.5.28 21:00 成都-&gt;北京 在生活中你我应该都有过这样的经历：自己的某种想法、某种情感、某种感受，找不到任何词句来形容，不论怎么组织语言，表达出来的东西似乎都离自己内心的那个点差了些什么。 你我应该也都有过这种体验：某时某刻，突然听到某个人说的一句话，某首歌的一节词，某篇文章的一些句子，突然直戳内心，道尽了你一直以来难以言表的某些东西，使你感到从内到外的深切共鸣。 夏目漱石曾有一个经典的比喻，他形容日语是高情境的语言，当日本人希望表达“我爱你”时，用“月色真美”将会更加恰当。木心说“文字的简练来源于内心的真诚，我十二万分的爱你，就不如说，我爱你”。 我们的思想、情感、感受、思维，应该是一种很高维度的东西，而相比之下，我们的语言是低维的。所以才会出现那么多的词不达意、难以言说。]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>思维 语言 夏目漱石 木心 维度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无题]]></title>
    <url>%2F2018%2F05%2F09%2F%E6%97%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[看着秋日四五点的太阳 总想到行将沉没的爱情 虽然还留有光辉的外表在强装 但我们都明晰地感觉到那里面已经冷了 或许必须承认的是其中还有温度吧 但我们都诚实地添了衣服 为迎接不远到来的寒夜 这看上去像是一种要自欺的态度 现实又冷厉得无可辩驳 此时沐浴在余晖下面 街上的行人是最线条分明的 多情者纵情开怀及时行乐 痴情者惘然自立冰冷不觉 无情者故自前行目不斜分 一时街道上疏影横斜泾渭明 前望后顾恍如一般景色 竟像是身立明镜之前 然陷身其内又不知左右何方是人何处是影 所以我讷讷然站在那里 唯恐撞上冰冷坚硬的镜面 身觉冰冷入袭却举步不得 落在旁人眼里 大概一如我所见的街上众人吧]]></content>
      <categories>
        <category>Emotion</category>
      </categories>
      <tags>
        <tag>脉络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：Faster R-CNN Features for Instance Search]]></title>
    <url>%2F2018%2F05%2F07%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9AFaster%20R-CNN%20Features%20for%20Instance%20Search%2F</url>
    <content type="text"><![CDATA[[CVPR 2016] Faster R-CNN Features for Instance SearchPaper: Link Abstract 研究基于CNN的目标检测网络得到的feature是否有利于image retrieval 研究对目标检测网络进行fine-tune后检索效果是否有所提升 实验数据集： Oxford Building 5K Paris Buildings 6K TRECVid Instance Search 2013 What they do 通过一次前向传播从CNN中提取出图像的全局和局部特征 利用从RPN中学到的位置信息设计空间重排方案 分析了对目标检测CNN进行fine-tune对检索的影响 Conclusion 利用目标检测的CNN特征可以用于图像检索 使用检索的图像对CNN进行微调，可以较大提升检索效果 IntroductionBackground 利用预训练的CNNs提取现成的特征，在图像检索通用数据集上获得了不错的效果 实例检索系统 = 快速筛选（对数据库进行重排） + 高级检索（一些高计算量的阶段）。 常见的重排方式有几何验证和空间分析 空间分析重排是指，使用不同尺寸的滑动窗口滑过图像，获得图像内部不同局部区域的特征表达，将这些局部特征与查询实例进行比对。 Related WorkCNNs for Instance Search 早期研究中利用全连接层的特征进行图像检索 将从不同图像sub-patches提取的全连接层特征进行结合，得到更好的结果 研究发现图像检索领域使用卷积层特征进行，比全连接层特征结果更好 Object Detection CNNs 早期使用滑动窗口方法 随后R-CNN采用Selective Search 现在使用端到端的方法 MethodologyCNN-based Representations 作者将查询实例定义为找到包含查询目标的边界框 提出两种池化方案： Image-wise pooling of activations(IPA):忽略操作目标建议的所有层，只从最后一个卷积层提取特征，对所有滤波器的输出进行求和 Region-wise pooling of activations(RPA)：利用区域池化层提取RPN得到的目标建议的特征。对于每个建议窗口，都可以利用其中RoI池化层的输出来生成表达。 求和池化需要进行L2标准化、白化、L2标准化，而最大池化仅仅进行一次L2标准化 Fine-tuning Faster R-CNN 作者提出了两种微调方案： 只更新分类这一分支的全连接层的权重 前两个卷积层之后的所有层都更新 Image Retrieval 作者提出检索由三步构成： 快速筛选 空间重排 扩展查询 快速筛选是利用IPA，即图像的全局表达进行一次过滤，只保留前N个结果 空间重排由两种方法： CA-SR：假设类别不可知，将查询实例经过网络得到的所有RPA，与快速筛选后剩下的每幅图的RPA进行比较 CS-SR：使用微调后的网络，可以用每个RPN预测的分类得分作为查询对象的相似分数 扩展查询策略为取重排后的前M个结果的特征向量，对他们进行求和平均，用平均后得到的结果重新进行一次查询 Notes图像检索实际上是把图片转为向量在高维空间中位置的比较，两个向量越近，距离越短，则说明越相似。直接使用从整张图片得到的特征向量进行比对的图片检索系统效果有很大的优化空间，因为图片中与检索目标无关的信息越多，就会加大向量在其他维度的权重，进而使得向量在空间中的位置发生偏移，与希望得到的结果的距离变大，这种现象随着查询目标在原图中所占比例的大小越小、显著性越低、对比越不强烈，检索偏差也会越大。 举一个比较简单而极端的例子：我们可以想象有一个10维的空间，在这个空间中，只有第1，2，3维的方向上代表了我们所查询的对象的表达（实际上可能每个维度上都有不同的权重，在这里我们极端地假设其他维度上权重为0）。通过CNN得到的特征向量，实际上是由图片内的内容共同组成的，与我们所查询的对象无关的内容也掺杂其中，这些无关内容在10维空间中标示为其他维度上的不同权重。 因此，图片全局的表达可以用作第一步的快速筛选，然后再使用局部表达精确检索。]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Image Retrieval</tag>
        <tag>CV</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
</search>
