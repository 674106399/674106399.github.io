<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2020%2F04%2F19%2F%E8%AE%BA%E6%96%87SubSpace%20Capsule%20Network%2F</url>
    <content type="text"><![CDATA[论文阅读：SubSpace Capsule Network摘要本文提出了子空间胶囊网络（SCN），该网络应用胶囊网络的思想，通过一组胶囊子空间对实体的外观或隐式定义的属性的可能变化进行建模，而不是简单地将神经元分组来创建胶囊。使用一个可学习的变换，通过将输入特征向量从较低层投影到胶囊子空间上来创建胶囊。 此变换找到输入与胶囊子空间建模的属性的相关程度。 本文展示的 SCN 是一个通用的胶囊网络，可以被应用于 GAN，在test 阶段不会产生多余的计算开销。通过使用生成对抗网络（GAN）框架对监督图像分类，半监督图像分类和高分辨率图像生成任务进行全面的实验，评估了SCN的有效性。 SCN显着提高了所有3个任务中基准模型的性能。 介绍一个胶囊是由一组神经元定义的，它们可以完备地对不同属性进行建模，例如姿态、纹理、一个实体的整体或局部形变等。胶囊网络的每一层由很多个胶囊组成。一个训练好的胶囊网络，每个胶囊的激活向量表示实体的实例化参数，且向量的长度表示那个特征或特征组件的存在可能性分数。本文尽管依旧遵循胶囊的主要定义，但提出的子空间胶囊网络是基于一个输入特征向量和一组学好的子空间之间的相关程度。在 SCN 中，每个专属的胶囊子空间是通过相关的实体或实体组件来学习的。然后通过使用基于对应胶囊子空间定义的学习变换，把输入向量投影到胶囊子空间中来创建胶囊。从直觉上讲，一个胶囊子空间捕捉了一个实体或实体组件的视觉属性的变换（如外观，姿态，纹理和形变等），一个子空间的输出特征的长度表示了该输入特征与该子空间所对应属性之间的相关程度。因此，如果一个子空间有一个很大的激活向量，这意味着该输入特征跟该子空间所对应的属性高度相关，反之亦然。这种创建子空间胶囊的形式与基于路由机制的形式是互相独立的，因此可以很容易地应用到大型网络中。 与本工作最相关的工作是胶囊投影网络（CapProNet），它极少量地应用了基于子空间的胶囊到图像分类网络的最后一层，且只需要胶囊的长度来进行预测。在一个 N 分类任务中，会学习一组胶囊子空间$${S_1,…,S_N}$$ ，每个类的胶囊通过输入骨干网络特征向量到每个子空间的正交投影来创建。输入图片归类到拥有最大胶囊长度的类别上。与 CapProNet 不同的是，SCN 同时对子空间胶囊和胶囊长度进行了研究。以下是本文的贡献： SCN 是一种通用的胶囊模型，可以直接应用到生成模型和判别模型中 SCN 计算量很小，在test 阶段不会增加额外的计算开销；在 train 阶段使用第五节的方法，增加的计算量也微乎其微 使用了 SCN 的 GAN ，生成的样本有至少20%的 FID score 提升 在 CIFAR10和 SVH 数据集的半监督分类任务中，SCN 达到了 SOAT，且两个数据集的提升均23%以上 SCN 很容易迁移到大型网络结构上。当应用到 ResNet 的最后一个 Block 上，Top1错误率有相对于之前5%的提升（PS：这里说法很 tricky，并不是准确率直接提升了5%，而是提升的比例相对于之前是5%） 模型架构 a) 生成器中，隐式表示 z 在第一层以 c=16维被投影到10个胶囊子空间中，具有最大的向量的胶囊被选中，然后变形为一个25x2x2的立方体，然后上采样来翻倍空间分辨率为4x4.该立方通过2层 sc-conv，每层都进行上采样操作来得到16x16的分辨率。最终的 sc-conv 层具有8个子空间胶囊类型，每个8维，这一层的输出会被变形来生成图片。 b)判别器中，使用6个卷积层提取特征，后面接3个 sc-conv 层，每层都含有64个子空间胶囊类型，1个子空间胶囊平均池化层和最终一个具有10个胶囊类型的子空间胶囊全连接层(sc-fc)。 SCN 的关键在于找到输入特征与胶囊子空间之间的相关程度，因此本文设计了投影矩阵和子空间，以及胶囊激活函数和子空间胶囊平均池化。 胶囊子空间投影对于layer k， 假如 x 是一个来自于 layer k-1的 d 维向量。假设一个 c 维的胶囊子空间 S 是由权重矩阵 W (d x c)的列的跨度构成的，c 远小于 d 最直观的方式来寻找特征向量 x 于胶囊子空间 S 的相关程度的方法，是把 x 正交投影到 S，该问题的一个近似解决方案是：$$y = W(W^TW)^{-1}W^Tx, P=W(W^TW)^{-1}W^T$$ ，P 是到 S 的正交投影矩阵，y 是 x 在 S 的投影。y 的长度越大，x 与 S 越相关，换言之，x 有越多的属性由 S 建模。 然而，投影矩阵 P 主要的缺点在于是一个正方形矩阵，这意味着如果我们创建一个 d 维特征 x 使用 P投影到 S得到的胶囊，那么这个胶囊也是在 d 维空间里的。实际上，在深度模型中 d 通常是很大的，使用正交投影矩阵 P 得到不同的胶囊类型会需要大量的内存。为了能够通过子空间胶囊层序列从各种大小的胶囊中受益，需要进行一种变换，来允许将输入特征向量 x 映射到胶囊子空间的 c 维空间，同时仍保留网络中连续的层的胶囊之间的关系。我们提出使用变换矩阵$$P_c$$ 代表的中间域来表示胶囊子空间。该矩由分解正交投影矩阵 P 得到： $P=P_dP_c….(2)$ $P_d=W(W^TW)^{-1/2}, P_c=(W^TW)^{-1/2}W^T$ 这里 $$P_c$$ 是把输入特征 x 映射到 c 维胶囊空间的变换，$$P_d$$ 是让胶囊空间中的投影向量回到原始的 d 维输入向量空间的变换。现在，与胶囊子空间 S 相关的胶囊可以通过将特征向量 x 投影到 c 维胶囊子空间得到： $u=P_cx….(4)$ 这里 u 代表 x 在胶囊空间中的低维表示。矩阵 P是一个半定对称矩阵。因此它的分解如式子(2)所示具有特殊属性。本文证明了使用$$P_c$$ 创建的胶囊具有相同的实例化参数信息和特征存在性分数，因为它是由变换 P 得到的，具体证明过程见原文。 激活函数本文基于对胶囊输出向量长度的解释，应用了两种激活函数在子空间胶囊上。胶囊输出向量的长度可以从置信度的角度解释。一个高置信度的胶囊说明输入向量跟该胶囊子空间是高度相关的。换言之，输入向量包含了该胶囊子空间建模的实体。本文也希望通过激活函数来抑制噪声胶囊的效果，从这个角度我们提出了”sparking”函数： $v = max(||u||-b^2, 0)\frac{u}{||u||}….(5)$ b 是一个可学习的参数。 直觉上，如果 x 跟子空间S 建模的实体相关，sparking 函数会提升胶囊确定性；如果它的长度小于阈值$$b^2$$，则完全关闭胶囊。初始时$$b^2=0.25$$ ，在训练时更新它。 另一种可能性是将由胶囊子空间建模的实体的存在概率与输出胶囊的长度相关联。对于这种情况，本文遵循 Hinton 提出的”squashing”函数： $v = \frac{||u||^2}{1+||u||^2}\frac{u}{||u||}….(6)$ 本文发现 sparking 函数在判别任务中更有效，比如半监督图像分类，因为它通过关闭噪声胶囊输出稀疏特征图加速了收敛。噪声胶囊是指每层中表示的属性与输入图片无关的胶囊，它们仍然会产生一个小的激活向量。然而在生成模型中，通过使用 squashing 激活函数来得到小而非零的值能提高生成样本的质量。 子空间胶囊卷积SCN 也可以从CNN权重共享的思维中受益，通过在图像的所有空间位置上使用相同的子空间胶囊类型。在子空间胶囊卷积中，如果输入 x 有 i 张特征图，而我们希望创建一个 感受野维 k 的c 维的子空间胶囊卷积核，我们需要基于权重矩阵 W（$$(i\times k \times k)\times c$$）构建变换矩阵$$P_c$$ 。我们可以把投影矩阵$$P_c$$的每一行看成一个 $$i \times k\times k$$ 的卷积核，对输入特征图进行卷积并生成输出胶囊的单个元素。所以如果$$P_c$$ 被重组到一个形如$$c \times i\times k \times k$$ 的4维张量，那么它可以被作为标准的卷积操作使用，且对应于每个空间位置的胶囊会被摆放在输出特征图的相应位置。现在，如果我们想有 n 个子空间胶囊类型，我们可以创建一组投影矩阵{$$P_{c1},…,P_{cn}$$} ，把它们分别重组成4维张量后拼接到一起来创建一个形如$$nc \times i \times k \times k$$ 的核，至此，我们使用一个四元组(n,c,k,k)来表示子空间胶囊卷积层 子空间胶囊平均池化平均池化的思想很自然地来自于子空间胶囊卷积。在子空间胶囊卷积中，相同类型的胶囊表示相同的视觉属性不论空间位置。因此可以安全地假设$$k \times k$$ 的小感受野中相同类型胶囊具有相似的方向，并且可以用一个它们的均值计算得到的胶囊来代表它们全部。 投影矩阵实现投影矩阵$$P_c$$包含计算量非常大的操作$$W^TW$$，如果不能正确实现，会极大拖慢训练过程。本文中，使用了Denman-Beavers 迭代方法的稳定扩展。对于任意的正定（半定）矩阵 A，存在一个唯一对称正定（半定）平方根矩阵。在所有训练过程中，我们设置迭代次数 k=20.这只极小地增加训练时间。 而在训练结束后，胶囊投影矩阵$$P_c$$就固定了，不再需要额外的时间。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F11%2F24%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9ASee%20Better%20Before%20Looking%20Closer%20Weakly%20Supervised%20Data%20Augmentation%20Network%20for%20Fine-Grained%20Visual%20Classification%2F</url>
    <content type="text"><![CDATA[See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual ClassificationAbstract数据增强被广泛用于提升训练数据的规模，避免过拟合，以及提升深度模型性能。然而在实际应用中，随机数据增强方法，比如随机图片裁剪，是比较低效的，且容易引入很多无法控制的背景噪声。 本文我们提出了一种弱监督数据增强网络(WS-DAN)来探索数据增强的潜力。具体来讲，对于每一张训练图片，我们首先通过弱监督学习方法生成attention maps 来表征对象的代表性区域。然后我们用 attention maps 来指导图片增强，包括 attention cropping 和 attention dropping. 我们提出的 WS-DAN 方法分两步提升了分类准确率。在第一阶段中，由于提取了更多具有代表性的区域的特征，图片可以被”看”得更好。在第二阶段中，注意力区域提供了准确的对象定位，这确保了我们的模型可以更”近”地观看图片并进一步提升性能。 在公共细粒度图像分类数据集上的综合实验显示，WS-DAN 超越了已有的 the-state-of-the-art方法。 Introduction数据增强是一种频繁使用的策略，通过增加数据多样性的方式增加训练数据，以提升深度模型的泛化能力。它已被证明在大多数的计算机视觉任务（如目标分类，检测，分割）中是非常有效的。然而，随机采样的裁剪区域有很大概率包含了很多背景噪声，这可能降低训练效率，影响特征提取的质量，进而抵消其产生的收益。为了提升数据增强的效率，模型应该知道目标对象的空间信息。 细粒度图像分类的目标是对某一基本类型下的子类进行分类，比如鸟的种类，车型，飞机型号等。细粒度图像分类极具挑战性，原因有三： 高类内差异。属于同一类别的对象通常呈现出明显不同的姿态和视角。 低类间差异。属于不同类别的对象相较于那些细小的区别，可能有的部位高度相似，比如一只鸟头部的颜色风格通常决定了其种类。 有限的训练数据。为细粒度种类打标通常需要专业知识，以及大量的标注时间。 基于以上原因，只使用最先进的粗分类模型（如 VGG，ResNet，Inception）很难得到高准确率。 在最近的工作中已经指出，细粒度图像分类的关键步骤在于从多个对象的部位中提取更具有代表性的局部特征。然而，”对象的部位”是很难定义的，且对象与对象之间差异很大。并且，为这些局部区域进行打标也需要大量额外人力。 在我们的工作中，我们利用弱监督学习只依靠图片级的标注来定位对象的代表性的部位。我们使用 attention maps 来表征对象的部位，而不是提出对象部位的兴趣 bbox。我们还提出双线性注意力池化（bilinear attention pooling）和注意力标准化损失函数（attentino regularization loss）来对注意力生成过程进行弱监督。我们的方法比其他进行对象部位定位的模型能更容易地定位大量对象的部位，因而取得了更好的性能。 获得对象部位的位置后，我们提出注意力指导的数据增强来高效地增加训练数据，并解决以上提到的高类内差异，低类间差异问题。]]></content>
  </entry>
  <entry>
    <title><![CDATA[线段树Python]]></title>
    <url>%2F2019%2F07%2F16%2F%E7%BA%BF%E6%AE%B5%E6%A0%91Python%2F</url>
    <content type="text"><![CDATA[最近在参加秋招，头条面试的时候遇到一道题感觉很契合实际场景，唤起了我对线段树的记忆题目是这样的： 给出一份服务器一天的log文件，包含用户id，登录时间，登出时间，求当天服务器最大在线人数是多少 当时看完这个题第一反应就是可以用线段树，但是线段树我以前只是了解了一下概念和适用场景，没有真正仔细研读，趁此机会算是好好学习了一下 线段树网上的教程一搜一大把，不过很少有Python版本的，能搜到的也都没有区间更新或者是维护节点和，而不是求最大值，所以把我的版本贴出来 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 定义树节点，l,r, val表示该节点记录的是区间[l, r]的最大值是valclass Tree(): def __init__(self): self.l = 0 self.r = 0 self.lazy = 0 self.val = 0 # 二叉树是堆形式，可以用一维数组存储，注意数组长度要开4倍空间tree = [Tree() for i in range(10*4)]# 建树，用cur&lt;&lt;1访问左子树，cur&lt;&lt;1|1访问右子树，位运算操作很方便def build(cur, l, r): tree[cur].l, tree[cur].r, tree[cur].lazy, tree[cur].val = l, r, 0, 0 # 当l==r的时候结束递归 if l &lt; r: mid = l + r &gt;&gt; 1 build(cur&lt;&lt;1, l, mid) build(cur&lt;&lt;1|1, mid+1, r)# 当子节点计算完成后，用子节点的值来更新自己的值def pushup(cur): tree[cur].val = max(tree[cur&lt;&lt;1].val, tree[cur&lt;&lt;1|1].val) # 单点更新def add(cur, x, v): if tree[cur].l == tree[cur].r: tree[cur].val += v else: mid = tree[cur].r + tree[cur].l &gt;&gt; 1 if x &gt; mid: add(cur&gt;&gt;1|1, x, v) else: add(cur&lt;&lt;1, x, v) pushup(cur)# 将lazy标记向下传递一层def pushdown(cur): if tree[cur].lazy: lazy = tree[cur].lazy tree[cur&lt;&lt;1].lazy += lazy tree[cur&lt;&lt;1|1].lazy += lazy tree[cur&lt;&lt;1].val += lazy tree[cur&lt;&lt;1|1].val += lazy tree[cur].lazy = 0# 区间更新def update(cur, l, r, v): if l &lt;= tree[cur].l and tree[cur].r &lt;= r: tree[cur].lazy += v tree[cur].val += v return if r &lt; tree[cur].l or l &gt; tree[cur].r: return if tree[cur].lazy: pushdown(cur) update(cur&lt;&lt;1, l, r, v) update(cur&lt;&lt;1|1, l, r, v) pushup(cur)# 区间查询def query(cur, l, r): if l &lt;= tree[cur].l and tree[cur].r &lt;= r: return tree[cur].val if tree[cur].l &gt; r or tree[cur].r &lt; l: return 0 if tree[cur].lazy: pushdown(cur) return max(query(cur&lt;&lt;1, l, r), query(cur&lt;&lt;1|1))# 测试# -----# ---# -------# --# --build(1, 1, 10)update(1, 1, 5, 1)update(1, 7, 10, 1)update(1, 2, 8, 1)update(1, 3, 4, 1)update(1, 9, 10, 1)print(query(1, 1, 10))# 返回为3]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记：细粒度级别图像分析：细粒度图像检索]]></title>
    <url>%2F2019%2F05%2F10%2F%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%BB%86%E7%B2%92%E5%BA%A6%E7%BA%A7%E5%88%AB%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[细粒度与粗粒度概念对比 粗粒度：类间appearance差异较大，如猫、鸟、鱼、飞机、汽车等 细粒度：类间差异较小，如不同品种的犬类之间，不同型号的飞机之间等 细粒度图像分类细粒度分类难点 apperance：类间差异较大（如猫和鸟），类内差异较小（如英短猫和美短猫） pose：类间差异可能较小（如不同品种，但相同姿态，相同视角布局），类内差异可能较大（如同一品种，但姿态、视角布局不同） 需要同时兼顾类间和类内差异造成的影响 细粒度图像分类关键找到分类对象的关键识别点 Method1: SCDA主要物体定位思路：用无监督的方法来定位主要物体，即用 pretrained 分类模型来获取特征图（如在 ImageNet 上训练的 VGG16） Notation Feature Map, Channel, Descriptor 从网络中间拿到的 h w d 的tensor可以看成 h * w 个 d 维的 向量 也可以看成 d 个 h * w 的 channel 每个 d 维向量也称为一个深度描述子（deep descriptor） Distributed Representation 每个 channel 会对物体不同部位进行响应，也有个别 channel 会对背景和噪声进行响应 Assumption： 绝大多数channel都会在主要物体上有所响应 因而很符合直觉地，我们可以从深度方向（d 方向）上对 channel 进行加和 一句话总结：对channel进行加和并设置阈值，过滤掉部分低响应区域，从而无监督地得到主要物体定位的 Mask Feature Aggregation由于背景和噪声会对检索造成负面影响，所以我们利用主要物体定位的 Mask把用于描述主要物体的 descriptor 筛选出来，然后进行特征聚合作为最终检索的特征向量 Notation VLAD, Fisher Vector, Pooling Approaches 特征聚合常用的三种方法，其中 VLAD 是一阶方法，FV 是二阶 经过实验对比，单纯的池化结果就比 VLAD 和 FV 更好 推测是因为深度特征本身就具有很高的非线性，再进行一阶或二阶的特征聚合会导致过拟合，反而使得结果劣化 我们将 avg pool 和 max pool 都结果进行级联后得到的特征称为 SCDA 后处理我们还可以对1024维的 SCDA 特征进行 Multi-layer ensumble，即提取网络不同深度的 tensor，以及图片水平翻转，进而得到 SCDA_flip 特征 笔者去 Github 上搜了一下 SCDA 的代码，找到一份 keras 版本的挺清晰的，贴在这里帮助加深理解 123456789101112131415161718192021222324252627282930def select_aggregate(feat_map, resize=None): A = tf.reduce_sum(feat_map, axis=-1, keepdims=True) a = tf.reduce_mean(A, axis=[1, 2], keepdims=True) M = tf.to_float(A&gt;a) if resize != None: M = tf.image.resize_images(M, resize) return Mdef scda_plus(map1, map2, alpha=1.0): _, h1, w1, _ = map1.shape M1 = select_aggregate(map1) M2 = select_aggregate(map2) S2 = map2 * M2 pavg2 = 1.0 / tf.reduce_sum(M2, axis=[1, 2]) * tf.reduce_sum(S2, axis=[1, 2]) # (b, d) pmax2 = tf.reduce_max(S2, axis=[1, 2]) # (b, d) S2 = tf.concat([pavg2, pmax2], axis=-1) # (b, 2d) # upsampling M2 = tf.image.resize_images(M2, [h1, w1], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) S1 = map1 * (M1 * M2) pavg1 = 1.0 / tf.reduce_sum(M1, axis=[1, 2]) * tf.reduce_sum(S1, axis=[1, 2]) pmax1 = tf.reduce_max(S1, axis=[1, 2]) S1 = tf.concat([pavg1, pmax1], axis=-1) Splus = tf.concat([S2, S1*alpha], axis=-1) # (b, 4d) Splus = tf.nn.l2_normalize(Splus, 0) return Splus 结果分析 对4096维的 SCDA 特征进行 SVD+whitening 处理后性能还会有进一步的提升 Method2: DDTImage Co-localization利用一个包含有某个共同物体的数据集，来对数据集中所有该物体进行定位。不需要知道这个物体的位置监督信息，甚至不需要知道这个物体具体是什么。 DDT（Deep Descriptor Trasforming） 利用 pretrained 网络对数据集所有图片提取tensor（每个 h w d 的tensor 都由 h * w 个深度描述子组成） 由于已知该数据集包含有某个共同物体，那么我们只需要衡量这 N h w 个 descriptor 之间的相关程度就可以了 因而我们只需要找到一个 mapping function，来把所有的 descriptor 映射到一个新的空间进行比较，在这个新空间中强相关的 descriptor 就应该是整个数据集中共同包含的物体对象所对应的描述子 PCA 技术可以很简单地做到这一点：PCA 实际上就是把一组 n 维向量映射到一组全新的 k 维正交向量上，所以我们可以把这些 d 维的深度描述子利用 PCA 降到1维，即只保留共同相关度最强的一个维度，在这个维度上的值就可以看做是该 descriptor 跟目标物体的相关程度 因此，我们可以利用这个构建好的 PCA 来将每张图片的 tensor，转化为一个 h * w 的矩阵 利用该方法，我们也可以轻松地找出数据集中一些噪声 笔者感觉 DDT 方法比 SCDA 更容易理解，Github 上也只搜到一个 caffe 版本的代码，就不贴出来了。个人感觉 DDT 的贡献主要是在定位方面，特征聚合和增强的方法仍可以参考 SCDA 的代码。 SCDA vs. DDT 通过对比可以发现，由于 SCDA 是基于单张图片的，所以很容易把一些与对象无关的内容也定位进去，而 DDT 由于得到了数据集中其他图片信息的协同定位，能更准确地定位对应的物体对象 DDT 的另一个优势在于省去了 SCDA 方法中的动态阈值计算，只需要以0来作为阈值 Benchmark 泛化性将不同无监督方法在6个 ImageNet 没有的类别数据上进行实验对比 DDT 方法的扩展[Collins et al., ECCV 2018] 对 tensor 进行NMF（非负矩阵分解）可以把物体的 key part 定位出来（如清真寺的穹顶，房屋主体等） 内容来源：旷视科技南京研究院负责人魏秀参：细粒度级别图像分析领域的现状与展望]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Image Retrieval</tag>
        <tag>CV</tag>
        <tag>Fine-Grained Image Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FAISS文档阅读（一）：预处理与后期处理]]></title>
    <url>%2F2019%2F05%2F10%2F%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E5%90%8E%E6%9C%9F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[预处理与后期处理预处理与后期处理用于重映射向量和id，把转换处理(transformations)应用到数据，然后用更好的索引来对搜索结果进行重新排序 映射Faiss ID默认情况下，Faiss分配一组id序列给被添加的向量来建立索引。 一些Index类会执行add_with_ids方法，该方法除了提供这些向量本身以外，还会提供64-bit的向量id。在搜索时，该类会返回存储好的id，而不是原始向量。 IndexIDMap这个索引会将另一个索引封装在内部，并在添加和搜索时对id进行转化。该索引维护了一张映射表。 举个例子： 12345index = faiss.IndexFlatL2(xb.shape[1]) ids = np.arange(xb.shape[0])index.add_with_ids(xb, ids) # 这里会报错，因为IndexFlatL2不支持add_with_idsindex2 = faiss.IndexIDMap(index)index2.add_with_ids(xb, ids) # 这里可以正常运行，向量被存储在下一层的索引中 IndexIVF中的IDIndexIVF子类总是存储向量ID，因此IndexIDMap的额外的映射表是浪费空间的。IndexIVF天生就提供了add_with_ids 预转换数据该操作通常在转换数据比建立索引更优先的时候很有用。Transformation类继承VectorTransform。一个VectorTransform类对输入的d_in维向量应用转换方法，并输出大小为d_out的向量。 转换方法 类名称 注释 随机旋转 RandomRotationMatrix 在建立IndexPQ或IndexLSH索引之前有助于重新平衡向量组成 维度重映射 RemapDimensionsTransform 当索引方式有要求的维度，或要对各种维度应用一个随机的排列方式时，减少或增加向量尺寸 PCA PCAMatrix 用于降维 OPQ旋转 OPQMatrix OPQ会对输入的向量进行旋转来使其在PQ编码时更合理。详情查看Optimized product quantization, Ge et al., CVPR’13 转换方法可以被一组向量训练，使用train方法，然后它们可以用apply方法来应用于另一组向量。 一种索引可以被封装于IndexPreTransform索引中来使得映射显式地进行，并使训练过程和索引训练综合进行。 示例：使用PCA来降维配合IndexPreTransform如果输入向量是2048维的，且不得不减少到16 bytes，那么事先用PCA来降维就是合理的。 1234567891011121314# IndexIVFPQ索引应该设定为256维，而不是2048维 coarse_quantizer = faiss.IndexFlatL2 (256) sub_index = faiss.IndexIVFPQ (coarse_quantizer, 256, ncoarse, 16, 8) # PCA 2048-&gt;256 # 在降维后进行了随机旋转(第四个参数) pca_matrix = faiss.PCAMatrix (2048, 256, 0, True) #- 封装索引 index = faiss.IndexPreTransform (pca_matrix, sub_index) # 也需要对PCA进行训练 index.train(...) # PCA要比添加操作更早执行 index.add(...) 示例：升高维数在往向量中插入0的时候，升高维数有时会很有用，这种方法对以下内容有帮助： 让维数d增大4倍 让维数d增大M倍]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Image Retrieval</tag>
        <tag>CV</tag>
        <tag>Deep Learning</tag>
        <tag>FAISS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带有返回值的多线程Python实例]]></title>
    <url>%2F2018%2F07%2F14%2Fthreads%2F</url>
    <content type="text"><![CDATA[计算密集型 vs. IO密集型我们根据任务对CPU的使用情况，可以把任务类型分为计算密集型和IO密集型，计算密集型任务需要大量占用计算资源，适合使用多进程执行，每个进程可以使用独立的CPU核心进行计算；IO密集型任务需要的计算量不大，但可能存在大量的IO、网络延迟等待，适合使用多线程执行。 有返回值的多线程实例Python初始的多线程对返回值的支持不是很好，我们可以通过类的继承来创建一个带有返回值的多线程类。 12345678910111213141516171819from threading import Thread class MyThread(Thread): def __init__(self, param1): Thread.__init__(self) self.param1 = param1 self.result = None def foo(self, param1): result = None # do something return result def run(self): self.result = self.foo(self.param1) def get_result(self): return self.result 然后我们就可以轻松地创建多线程： 123456789t_list = []for i in range(3): t = MyThread(i) t_list.append(t) t.start() for t in t_list: t.join() print(t.get_result()) 启动之后一定要t.join()将主程序阻塞，等待子进程运行结束，不然主线程比子线程跑的快，会拿不到结果。]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Multiprocessing实用技能总结]]></title>
    <url>%2F2018%2F07%2F13%2Fmultiprocessing%2F</url>
    <content type="text"><![CDATA[为什么要使用Python多进程多线程并发具有内存共享、通信简单等优势，那么为什么我们还要选择多进程呢？下面我引用廖雪峰的Python教程中的一段说明： 因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。 GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。 所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。 不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 多进程的优势我们有很多的任务是相互独立运行的，也有很多任务实际上是可以通过拆分之后并行加速的。比如一个很大的list需要挨个遍历处理其中的元素，比如一组数据需要分别访问不同的方法处理得到不同的结果等等。 如果使用for loop来调用执行，实际上是一种串行的方式，必须要等待上一个任务结束才能执行下一个，中间会有大量的不必要的等待时间。 Pool我们使用进程池multiprocessing.Pool()来自动管理进程任务，首先要初始化进程池。 Pool方法接收一个参数MAX_TASKS，用于设置该进程池最多允许多少个子进程同时执行，若进程池中的子进程数大于MAX_TASKS，则剩余的子进程会进入等待，每当有子进程执行完毕，就会有新的子进程启动补充进去： 123from multiprocessing import PoolMAX_TASKS = 5pool = Pool(MAX_TASKS) 使用多进程可以加速计算的根本原因在于，可以利用多核CPU来分别执行不同的任务，因此最好先了解自己的CPU有多少个核心： 12# 该语句可以获取自己CPU的核心数NUM_CPUS = multiprocessing.cpu_count() 为避免多余的任务切换开销，一般MAX_TASKS不要大于NUM_CPUS。 然后我们需要定义一个function，该方法会被分配给每个新进程然后执行： 123def child_task(param1, param2): # do something return result 然后我们可以在主进程中调用apply_async方法来创建新进程并加入进程池中。 apply_async方法接受两个参数，第一个是要调用执行的方法，第二个是传递给该方法的参数： 1234567891011NUM_TASKS = 10results = []for i in range(NUM_TASKS): r = pool.apply_async(child_task, args=(param1, param2)) results.append(r)p.close() # 进程池停止接收新进程p.join() # 阻塞主进程，等待所有子进程结束for r in results: print(r.get()) 需要注意到上面的代码中，r.get()需要放到p.close()和p.join()执行进程池回收之后再使用。这是因为apply_async之后的语句是阻塞的，r.get()会等待上一条语句执行完毕才执行，因此获取返回值的过程最好放在进程池回收以后。]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>multiprocessing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用FPN和ROIalign来进行图像检索]]></title>
    <url>%2F2018%2F06%2F28%2F2018-6-28%2F</url>
    <content type="text"><![CDATA[上一版的模型出来以后测试发现一个问题，同一张图，低分辨率和高分辨率，检索结果完全不同。并且最气的是，这两张图相互查不到对方，模型输出的两个向量距离很远，这是不可接受的。 这个问题其实不难想到，深度模型每一层卷积都会对特征图进行压缩，在模型内部自然地形成一个金字塔形的结构（哪怕有padding操作，其实也只是在维持特征图尺寸，实际的信息依然被压缩，这是不可避免的）。就像一个逐渐抽象的过程中，势必要丢掉一些冗余信息，才能抓取到代表性的特征。 于是乎，当输入的图片本身分辨率很低，即含有的信息量很少时，这种深层压缩的结果会是毁灭性的。可以想象成从飞机上俯视地面上的一个人，你看到的或许仅仅是一个肉色的小色块了。 这个问题在目标检测中同样存在，之前流行的主流算法都是直接使用骨干网络的最后一层卷积图作为信息的载体，因此，当目标在图中的比例非常小时，往往很难被算法检测到。针对这个问题，Kaiming He大神提出了一个很厉害的通用结构FPN，仅仅是把它直接结合到Faster RCNN上，就让检测的召回率大大提升。 一般而言，不同阶段的卷积层输出含有不同层次的空间和语义信息，直接从模型的每一段提出来的卷积图是不能混合使用的。FPN通过top-down和横向连接，以及一个3*3的smooth layer解决了这个问题，使不同阶段的特征图语义统一成为现实。 对于低分辨率的图片，通过把深层卷积图上采样叠加到浅层卷积图上，可以在保留高层次语义信息的基础上，极大地保留空间信息。 k = floor(4 + sqrt(w * h) / 224) 通过这个公式，可以自动根据输入图片的尺寸来调整获取对应尺度的特征图。 这里有一个疑惑是，很明显在FPN结构中，P2会是该结构受益最大的层，因为它整合了所有阶段的卷积图信息，包含了最丰富的空间和语义信息。在Paper的消融实验里也证实，只使用P2卷积图就能得到非常好的效果。那么为什么还要使用不同阶段的特征图？或者说，何不直接对所有尺度的图片使用P2特征图？ 我的猜测是，也许对于高分辨率图片，其包含的空间信息过于庞大，即使通过深层模型依然可以有所保留，因而不需要再从浅层卷积图获取，这种特征图叠加融合的手段，主要目的还是在改善低分辨率的性能，是一种补偿手段，在高分辨率图上反而会因信息过多而再次带来语义的不统一，这是一种空间与语义的权衡折中。 从直觉上，通过在图片上建立FPN，再选择合适的尺度特征，就可以用来进行图像检索。相较于目标检测中有RPN提供ROI，在图像检索中只能以整张图作为输入，与ROI类似的，图片的比例也往往不是标准的1:1，提取得到的特征图也是如此，直接插值放缩是不可行的，会破坏原有的空间结构，因此还需要引入ROI池化来使得特征维度统一。根据Mask RCNN的研究，ROI池化存在各种各样的不足之处，ROIalign是一个更好的策略。 图像检索不同于目标检测的另一点是，ROI的边界往往是紧贴检测对象的，换言之，目标会充满整个ROI；而图片并非如此，往往还会存在大量的留白区域，因此直接对全图进行ROIalign同样会导致得到的特征的不一致，这里我才用了以下策略来对要输入模型的图片进行预处理： 1234567# 伪代码# 获取长边pad = img.width if img.width &gt; img.height else img.height# 用长边对图片进行扩充Padding(img)# 从图片中心截取pad*pad的区域CenterCrop(img, pad) 如此一来，我们就得到了将短边延拓到长边尺寸的图片。我们在这个图片的基础上进行FPN和ROIalign得到最终的特征向量。]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>FPN</tag>
        <tag>ROIalign</tag>
        <tag>Image Retrieval</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随笔：飞机上1]]></title>
    <url>%2F2018%2F05%2F28%2F2018-5-28%2F</url>
    <content type="text"><![CDATA[2018.5.28 21:00 成都-&gt;北京 在生活中你我应该都有过这样的经历：自己的某种想法、某种情感、某种感受，找不到任何词句来形容，不论怎么组织语言，表达出来的东西似乎都离自己内心的那个点差了些什么。 你我应该也都有过这种体验：某时某刻，突然听到某个人说的一句话，某首歌的一节词，某篇文章的一些句子，突然直戳内心，道尽了你一直以来难以言表的某些东西，使你感到从内到外的深切共鸣。 夏目漱石曾有一个经典的比喻，他形容日语是高情境的语言，当日本人希望表达“我爱你”时，用“月色真美”将会更加恰当。木心说“文字的简练来源于内心的真诚，我十二万分的爱你，就不如说，我爱你”。 我们的思想、情感、感受、思维，应该是一种很高维度的东西，而相比之下，我们的语言是低维的。所以才会出现那么多的词不达意、难以言说。]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>思维 语言 夏目漱石 木心 维度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：Faster R-CNN Features for Instance Search]]></title>
    <url>%2F2018%2F05%2F07%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9AFaster%20R-CNN%20Features%20for%20Instance%20Search%2F</url>
    <content type="text"><![CDATA[[CVPR 2016] Faster R-CNN Features for Instance SearchPaper: Link Abstract 研究基于CNN的目标检测网络得到的feature是否有利于image retrieval 研究对目标检测网络进行fine-tune后检索效果是否有所提升 实验数据集： Oxford Building 5K Paris Buildings 6K TRECVid Instance Search 2013 What they do 通过一次前向传播从CNN中提取出图像的全局和局部特征 利用从RPN中学到的位置信息设计空间重排方案 分析了对目标检测CNN进行fine-tune对检索的影响 Conclusion 利用目标检测的CNN特征可以用于图像检索 使用检索的图像对CNN进行微调，可以较大提升检索效果 IntroductionBackground 利用预训练的CNNs提取现成的特征，在图像检索通用数据集上获得了不错的效果 实例检索系统 = 快速筛选（对数据库进行重排） + 高级检索（一些高计算量的阶段）。 常见的重排方式有几何验证和空间分析 空间分析重排是指，使用不同尺寸的滑动窗口滑过图像，获得图像内部不同局部区域的特征表达，将这些局部特征与查询实例进行比对。 Related WorkCNNs for Instance Search 早期研究中利用全连接层的特征进行图像检索 将从不同图像sub-patches提取的全连接层特征进行结合，得到更好的结果 研究发现图像检索领域使用卷积层特征进行，比全连接层特征结果更好 Object Detection CNNs 早期使用滑动窗口方法 随后R-CNN采用Selective Search 现在使用端到端的方法 MethodologyCNN-based Representations 作者将查询实例定义为找到包含查询目标的边界框 提出两种池化方案： Image-wise pooling of activations(IPA):忽略操作目标建议的所有层，只从最后一个卷积层提取特征，对所有滤波器的输出进行求和 Region-wise pooling of activations(RPA)：利用区域池化层提取RPN得到的目标建议的特征。对于每个建议窗口，都可以利用其中RoI池化层的输出来生成表达。 求和池化需要进行L2标准化、白化、L2标准化，而最大池化仅仅进行一次L2标准化 Fine-tuning Faster R-CNN 作者提出了两种微调方案： 只更新分类这一分支的全连接层的权重 前两个卷积层之后的所有层都更新 Image Retrieval 作者提出检索由三步构成： 快速筛选 空间重排 扩展查询 快速筛选是利用IPA，即图像的全局表达进行一次过滤，只保留前N个结果 空间重排由两种方法： CA-SR：假设类别不可知，将查询实例经过网络得到的所有RPA，与快速筛选后剩下的每幅图的RPA进行比较 CS-SR：使用微调后的网络，可以用每个RPN预测的分类得分作为查询对象的相似分数 扩展查询策略为取重排后的前M个结果的特征向量，对他们进行求和平均，用平均后得到的结果重新进行一次查询 Notes图像检索实际上是把图片转为向量在高维空间中位置的比较，两个向量越近，距离越短，则说明越相似。直接使用从整张图片得到的特征向量进行比对的图片检索系统效果有很大的优化空间，因为图片中与检索目标无关的信息越多，就会加大向量在其他维度的权重，进而使得向量在空间中的位置发生偏移，与希望得到的结果的距离变大，这种现象随着查询目标在原图中所占比例的大小越小、显著性越低、对比越不强烈，检索偏差也会越大。 举一个比较简单而极端的例子：我们可以想象有一个10维的空间，在这个空间中，只有第1，2，3维的方向上代表了我们所查询的对象的表达（实际上可能每个维度上都有不同的权重，在这里我们极端地假设其他维度上权重为0）。通过CNN得到的特征向量，实际上是由图片内的内容共同组成的，与我们所查询的对象无关的内容也掺杂其中，这些无关内容在10维空间中标示为其他维度上的不同权重。 因此，图片全局的表达可以用作第一步的快速筛选，然后再使用局部表达精确检索。]]></content>
      <categories>
        <category>Technology</category>
      </categories>
      <tags>
        <tag>Image Retrieval</tag>
        <tag>CV</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
</search>
