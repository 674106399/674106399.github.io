<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>笔记：细粒度级别图像分析：细粒度图像检索 | 镜子的掌纹</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">笔记：细粒度级别图像分析：细粒度图像检索</h1><a id="logo" href="/.">镜子的掌纹</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">笔记：细粒度级别图像分析：细粒度图像检索</h1><div class="post-meta">May 10, 2019<span> | </span><span class="category"><a href="/categories/Technology/">Technology</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="post-content"><h1 id="细粒度与粗粒度概念对比"><a href="#细粒度与粗粒度概念对比" class="headerlink" title="细粒度与粗粒度概念对比"></a>细粒度与粗粒度概念对比</h1><p><img src="/Users/billow/Desktop/1.png" alt=""></p>
<ul>
<li><p><strong>粗粒度</strong>：类间appearance差异较大，如猫、鸟、鱼、飞机、汽车等</p>
</li>
<li><p><strong>细粒度</strong>：类间差异较小，如不同品种的犬类之间，不同型号的飞机之间等</p>
</li>
</ul>
<h1 id="细粒度图像分类"><a href="#细粒度图像分类" class="headerlink" title="细粒度图像分类"></a>细粒度图像分类</h1><h2 id="细粒度分类难点"><a href="#细粒度分类难点" class="headerlink" title="细粒度分类难点"></a>细粒度分类难点</h2><ul>
<li><strong>apperance</strong>：类间差异较大（如猫和鸟），类内差异较小（如英短猫和美短猫）</li>
<li><strong>pose</strong>：类间差异可能较小（如不同品种，但相同姿态，相同视角布局），类内差异可能较大（如同一品种，但姿态、视角布局不同）</li>
</ul>
<p>需要同时兼顾类间和类内差异造成的影响</p>
<h2 id="细粒度图像分类关键"><a href="#细粒度图像分类关键" class="headerlink" title="细粒度图像分类关键"></a>细粒度图像分类关键</h2><p><img src="/Users/billow/Desktop/2.png" alt=""><br>找到分类对象的关键识别点</p>
<h2 id="Method1-SCDA"><a href="#Method1-SCDA" class="headerlink" title="Method1: SCDA"></a>Method1: SCDA</h2><h3 id="主要物体定位"><a href="#主要物体定位" class="headerlink" title="主要物体定位"></a>主要物体定位</h3><p>思路：用无监督的方法来定位主要物体，即用 pretrained 分类模型来获取特征图（如在 ImageNet 上训练的 VGG16）</p>
<blockquote>
<p>Notation</p>
<p><strong>Feature Map, Channel, Descriptor</strong></p>
<p><img src="/Users/billow/Desktop/3.png" alt=""><br>从网络中间拿到的 h <em> w </em> d 的tensor可以看成 h * w 个 d 维的 向量</p>
<p>也可以看成 d 个 h * w 的 channel</p>
<p>每个 d 维向量也称为一个深度描述子（deep descriptor）</p>
<p><strong>Distributed Representation</strong></p>
<p><img src="/Users/billow/Desktop/4.png" alt=""></p>
<p>每个 channel 会对物体不同部位进行响应，也有个别 channel 会对背景和噪声进行响应</p>
<p>Assumption：</p>
<p>绝大多数channel都会在主要物体上有所响应</p>
<p>因而很符合直觉地，我们可以从深度方向（d 方向）上对 channel 进行加和</p>
</blockquote>
<p><strong>一句话总结：对channel进行加和并设置阈值，过滤掉部分低响应区域，从而无监督地得到主要物体定位的 Mask</strong></p>
<p><img src="/Users/billow/Desktop/13.png" alt=""></p>
<h3 id="Feature-Aggregation"><a href="#Feature-Aggregation" class="headerlink" title="Feature Aggregation"></a>Feature Aggregation</h3><p>由于背景和噪声会对检索造成负面影响，所以我们利用主要物体定位的 Mask把用于描述主要物体的 descriptor 筛选出来，然后进行特征聚合作为最终检索的特征向量</p>
<blockquote>
<p>Notation</p>
<p><strong>VLAD, Fisher Vector, Pooling Approaches</strong></p>
<p>特征聚合常用的三种方法，其中 VLAD 是一阶方法，FV 是二阶</p>
<p><img src="/Users/billow/Desktop/5.png" alt=""></p>
<p>经过实验对比，单纯的池化结果就比 VLAD 和 FV 更好</p>
<p>推测是因为深度特征本身就具有很高的非线性，再进行一阶或二阶的特征聚合会导致过拟合，反而使得结果劣化</p>
</blockquote>
<p>我们将 avg pool 和 max pool 都结果进行级联后得到的特征称为 SCDA</p>
<h3 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h3><p>我们还可以对1024维的 SCDA 特征进行 Multi-layer ensumble，即提取网络不同深度的 tensor，以及图片水平翻转，进而得到 SCDA_flip 特征</p>
<blockquote>
<p>笔者去 Github 上搜了一下 SCDA 的代码，找到一份 keras 版本的挺清晰的，贴在这里帮助加深理解</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def select_aggregate(feat_map, resize=None):</span><br><span class="line">    A = tf.reduce_sum(feat_map, axis=-1, keepdims=True)</span><br><span class="line">    a = tf.reduce_mean(A, axis=[1, 2], keepdims=True)</span><br><span class="line">    M = tf.to_float(A&gt;a)</span><br><span class="line">    if resize != None:</span><br><span class="line">        M = tf.image.resize_images(M, resize)</span><br><span class="line">    return M</span><br><span class="line"></span><br><span class="line">def scda_plus(map1, map2, alpha=1.0):</span><br><span class="line">    _, h1, w1, _ = map1.shape</span><br><span class="line">    </span><br><span class="line">    M1 = select_aggregate(map1)</span><br><span class="line">    M2 = select_aggregate(map2)</span><br><span class="line"></span><br><span class="line">    S2 = map2 * M2</span><br><span class="line">    pavg2 = 1.0 / tf.reduce_sum(M2, axis=[1, 2]) * tf.reduce_sum(S2, axis=[1, 2]) # (b, d)</span><br><span class="line">    pmax2 = tf.reduce_max(S2, axis=[1, 2]) # (b, d)</span><br><span class="line">    S2 = tf.concat([pavg2, pmax2], axis=-1) # (b, 2d)</span><br><span class="line"></span><br><span class="line">    # upsampling</span><br><span class="line">    M2 = tf.image.resize_images(M2, [h1, w1], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</span><br><span class="line">    S1 = map1 * (M1 * M2)</span><br><span class="line">    pavg1 = 1.0 / tf.reduce_sum(M1, axis=[1, 2]) * tf.reduce_sum(S1, axis=[1, 2])</span><br><span class="line">    pmax1 = tf.reduce_max(S1, axis=[1, 2]) </span><br><span class="line">    S1 = tf.concat([pavg1, pmax1], axis=-1)</span><br><span class="line"></span><br><span class="line">    Splus = tf.concat([S2, S1*alpha], axis=-1) # (b, 4d)</span><br><span class="line">    Splus = tf.nn.l2_normalize(Splus, 0)</span><br><span class="line"></span><br><span class="line">    return Splus</span><br></pre></td></tr></table></figure>
<h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p><img src="/Users/billow/Desktop/12.png" alt=""></p>
<p>对4096维的 SCDA 特征进行 SVD+whitening 处理后性能还会有进一步的提升</p>
<h2 id="Method2-DDT"><a href="#Method2-DDT" class="headerlink" title="Method2: DDT"></a>Method2: DDT</h2><h3 id="Image-Co-localization"><a href="#Image-Co-localization" class="headerlink" title="Image Co-localization"></a>Image Co-localization</h3><p>利用一个包含有某个共同物体的数据集，来对数据集中所有该物体进行定位。不需要知道这个物体的位置监督信息，甚至不需要知道这个物体具体是什么。</p>
<h3 id="DDT（Deep-Descriptor-Trasforming）"><a href="#DDT（Deep-Descriptor-Trasforming）" class="headerlink" title="DDT（Deep Descriptor Trasforming）"></a>DDT（Deep Descriptor Trasforming）</h3><p><img src="/Users/billow/Desktop/6.png" alt=""></p>
<p>利用 pretrained 网络对数据集所有图片提取tensor（每个 h <em> w </em> d 的tensor 都由 h * w 个深度描述子组成）</p>
<p>由于已知该数据集包含有某个共同物体，那么我们只需要衡量这 N <em> h </em> w 个 descriptor 之间的相关程度就可以了</p>
<p>因而我们只需要找到一个 mapping function，来把所有的 descriptor 映射到一个新的空间进行比较，在这个新空间中强相关的 descriptor 就应该是整个数据集中共同包含的物体对象所对应的描述子</p>
<p>PCA 技术可以很简单地做到这一点：PCA 实际上就是把一组 n 维向量映射到一组全新的 k 维正交向量上，所以我们可以把这些 d 维的深度描述子利用 PCA 降到1维，即只保留<strong>共同</strong>相关度最强的一个维度，<strong>在这个维度上的值就可以看做是该 descriptor 跟目标物体的相关程度</strong></p>
<p>因此，我们可以利用这个构建好的 PCA 来将每张图片的 tensor，转化为一个 h * w 的矩阵</p>
<p><img src="/Users/billow/Desktop/8.png" alt=""></p>
<p>利用该方法，我们也可以轻松地找出数据集中一些噪声</p>
<blockquote>
<p>笔者感觉 DDT 方法比 SCDA 更容易理解，Github 上也只搜到一个 caffe 版本的代码，就不贴出来了。个人感觉 DDT 的贡献主要是在定位方面，特征聚合和增强的方法仍可以参考 SCDA 的代码。</p>
</blockquote>
<h2 id="SCDA-vs-DDT"><a href="#SCDA-vs-DDT" class="headerlink" title="SCDA vs. DDT"></a>SCDA vs. DDT</h2><p><img src="/Users/billow/Desktop/7.png" alt=""></p>
<p>通过对比可以发现，由于 SCDA 是基于单张图片的，所以很容易把一些与对象无关的内容也定位进去，而 DDT 由于得到了数据集中其他图片信息的协同定位，能更准确地定位对应的物体对象</p>
<p>DDT 的另一个优势在于省去了 SCDA 方法中的动态阈值计算，只需要以0来作为阈值</p>
<h3 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h3><p><img src="/Users/billow/Desktop/9.png" alt=""></p>
<h3 id="泛化性"><a href="#泛化性" class="headerlink" title="泛化性"></a>泛化性</h3><p>将不同无监督方法在6个 ImageNet 没有的类别数据上进行实验对比<br><img src="/Users/billow/Desktop/10.png" alt=""></p>
<h2 id="DDT-方法的扩展"><a href="#DDT-方法的扩展" class="headerlink" title="DDT 方法的扩展"></a>DDT 方法的扩展</h2><p><img src="/Users/billow/Desktop/11.png" alt=""><br>[Collins et al., ECCV 2018]</p>
<p>对 tensor 进行NMF（非负矩阵分解）可以把物体的 key part 定位出来（如清真寺的穹顶，房屋主体等）</p>
<h2 id="内容来源："><a href="#内容来源：" class="headerlink" title="内容来源："></a>内容来源：</h2><p><a href="https://www.bilibili.com/video/av43499197/" target="_blank" rel="noopener">旷视科技南京研究院负责人魏秀参：细粒度级别图像分析领域的现状与展望</a></p>
</div><div class="tags"><a href="/tags/Image-Retrieval/">Image Retrieval</a><a href="/tags/CV/">CV</a><a href="/tags/Fine-Grained-Image-Analysis/">Fine-Grained Image Analysis</a></div><div class="post-nav"><a class="pre" href="/2019/07/16/线段树Python/">线段树Python</a><a class="next" href="/2019/05/10/预处理与后期处理/">FAISS文档阅读（一）：预处理与后期处理</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Notes/">Notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technology/">Technology</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/思维-语言-夏目漱石-木心-维度/" style="font-size: 15px;">思维 语言 夏目漱石 木心 维度</a> <a href="/tags/ROIalign/" style="font-size: 15px;">ROIalign</a> <a href="/tags/Image-Retrieval/" style="font-size: 15px;">Image Retrieval</a> <a href="/tags/CV/" style="font-size: 15px;">CV</a> <a href="/tags/Fine-Grained-Image-Analysis/" style="font-size: 15px;">Fine-Grained Image Analysis</a> <a href="/tags/FPN/" style="font-size: 15px;">FPN</a> <a href="/tags/multiprocessing/" style="font-size: 15px;">multiprocessing</a> <a href="/tags/Thread/" style="font-size: 15px;">Thread</a> <a href="/tags/Algorithm/" style="font-size: 15px;">Algorithm</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/FAISS/" style="font-size: 15px;">FAISS</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/04/19/论文SubSpace Capsule Network/">论文SubSpace Capsule Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/24/论文阅读笔记：See Better Before Looking Closer Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification/">论文阅读笔记：See Better Before Looking Closer Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/16/线段树Python/">线段树Python</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/10/笔记：细粒度级别图像分析/">笔记：细粒度级别图像分析：细粒度图像检索</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/10/预处理与后期处理/">FAISS文档阅读（一）：预处理与后期处理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/14/threads/">带有返回值的多线程Python实例</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/13/multiprocessing/">Multiprocessing实用技能总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/28/2018-6-28/">用FPN和ROIalign来进行图像检索</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/28/2018-5-28/">随笔：飞机上1</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/07/论文阅读笔记：Faster R-CNN Features for Instance Search/">论文笔记：Faster R-CNN Features for Instance Search</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/674106399" title="My Github" target="_blank">My Github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">镜子的掌纹.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>